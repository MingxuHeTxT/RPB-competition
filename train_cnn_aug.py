#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
train_cnn_aug.py

1. åŠ è½½å¹¶å¢å¼ºæ•°æ®ï¼šéšæœºæ—‹è½¬ã€æ°´å¹³ç¿»è½¬ã€å¡«å……è£å‰ª â†’ ToTensor â†’ å½’ä¸€åŒ–
2. å®šä¹‰ CNN ç½‘ç»œ
3. è®­ç»ƒ & éªŒè¯å¾ªç¯
"""

import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms

# â”€â”€â”€ 1. æ•°æ®å¢å¼º & Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# å®šä¹‰å¢å¼ºç®¡é“
augment = transforms.Compose([
    transforms.RandomRotation(10),          # Â±10Â° éšæœºæ—‹è½¬
    transforms.RandomHorizontalFlip(),      # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomCrop(24, padding=2),   # éšæœºè£å‰ªåˆ° 24Ã—24
    transforms.ToTensor(),                  # è½¬æˆ [0,1] ä¹‹é—´çš„å¼ é‡ CÃ—HÃ—W
    transforms.Normalize((0.5,), (0.5,)),   # å½’ä¸€åŒ–åˆ° [-1,1]
])

class RPSAugDataset(Dataset):
    def __init__(self, pkl_path, transform=None):
        data = pickle.load(open(pkl_path, 'rb'))
        self.imgs1 = np.stack(data['img1'])  # (N,24,24)
        self.imgs2 = np.stack(data['img2'])
        labels = np.array(data['label'])     # (+1, -1)
        # è½¬æˆ 0/1
        self.labels = torch.tensor((labels == 1).astype(np.int64))
        self.transform = transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # å–å‡º numpy ç°åº¦å›¾å¹¶è½¬ PIL
        im1 = Image.fromarray(self.imgs1[idx].astype(np.uint8))
        im2 = Image.fromarray(self.imgs2[idx].astype(np.uint8))
        # åº”ç”¨åŒæ ·çš„å¢å¼º
        if self.transform:
            im1 = self.transform(im1)   # Tensor 1Ã—24Ã—24
            im2 = self.transform(im2)
        # æ‹¼æˆ 2 é€šé“
        x = torch.cat([im1, im2], dim=0)  # -> 2Ã—24Ã—24
        y = self.labels[idx]
        return x, y

# â”€â”€â”€ 2. ç®€å• CNN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(2, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool  = nn.MaxPool2d(2,2)
        self.fc1   = nn.Linear(64*6*6, 128)
        self.fc2   = nn.Linear(128, 2)
        self.drop  = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # 24â†’12
        x = self.pool(F.relu(self.conv2(x)))  # 12â†’6
        x = x.view(x.size(0), -1)
        x = self.drop(F.relu(self.fc1(x)))
        return self.fc2(x)

# â”€â”€â”€ 3. è®­ç»ƒ & éªŒè¯å¾ªç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def train_one_epoch(model, loader, crit, opt, device):
    model.train()
    loss_sum, correct, total = 0, 0, 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)
        opt.zero_grad()
        logits = model(X)
        loss = crit(logits, y)
        loss.backward()
        opt.step()
        loss_sum += loss.item() * X.size(0)
        preds = logits.argmax(dim=1)
        correct += (preds==y).sum().item()
        total += y.size(0)
    return loss_sum/total, correct/total

def validate(model, loader, crit, device):
    model.eval()
    loss_sum, correct, total = 0, 0, 0
    with torch.no_grad():
        for X, y in loader:
            X, y = X.to(device), y.to(device)
            logits = model(X)
            loss = crit(logits, y)
            loss_sum += loss.item() * X.size(0)
            preds = logits.argmax(dim=1)
            correct += (preds==y).sum().item()
            total += y.size(0)
    return loss_sum/total, correct/total

def main():
    # è¶…å‚
    batch_size = 64
    lr = 5e-4
    epochs = 15

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ•°æ®é›† & åˆ’åˆ†
    full_ds = RPSAugDataset('train.pkl', transform=augment)
    train_n = int(len(full_ds)*0.8)
    val_n   = len(full_ds) - train_n
    train_ds, val_ds = random_split(full_ds, [train_n, val_n])
    tr_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    va_loader = DataLoader(val_ds, batch_size=batch_size)

    # æ¨¡å‹ & ä¼˜åŒ–å™¨ & æŸå¤±
    model = SimpleCNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # è®­ç»ƒå¾ªç¯
    best_va_acc = 0.
    for ep in range(1, epochs+1):
        tr_loss, tr_acc = train_one_epoch(model, tr_loader, criterion, optimizer, device)
        va_loss, va_acc = validate(model, va_loader, criterion, device)
        print(f"Epoch {ep:2d} | "
              f"Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.4f} | "
              f"Val   Loss: {va_loss:.4f}, Acc: {va_acc:.4f}")
        best_va_acc = max(best_va_acc, va_acc)

    print(f"\nğŸ‰ æ•°æ®å¢å¼ºåæœ€ä½³éªŒè¯ Acc: {best_va_acc:.4f}")

if __name__ == '__main__':
    main()
