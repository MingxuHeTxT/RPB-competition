{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyN3jTHFXboEDyYlczwQEpYz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"2A0I7AEW_dij","executionInfo":{"status":"ok","timestamp":1746316699339,"user_tz":240,"elapsed":3,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}},"outputId":"d87622f2-9287-43ef-eaea-d6431242fe78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive/\\')\\nimport os\\nproject_path = \\'/content/drive/MyDrive/Cornell/pvz\\'\\nos.chdir(project_path)\\nprint(\"å½“å‰å·¥ä½œè·¯å¾„ï¼š\", os.getcwd())\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}],"source":["'''\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","import os\n","project_path = '/content/drive/MyDrive/Cornell/pvz'\n","os.chdir(project_path)\n","print(\"å½“å‰å·¥ä½œè·¯å¾„ï¼š\", os.getcwd())\n","'''"]},{"cell_type":"code","source":["'''\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader, random_split, Subset\n","from torchvision import transforms, models\n","import torchvision.transforms.functional as TF\n","from sklearn.model_selection import StratifiedKFold\n","import os\n","import time\n","'''"],"metadata":{"id":"D3betHL9QwQo","executionInfo":{"status":"ok","timestamp":1746316699377,"user_tz":240,"elapsed":8,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}},"colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"b219fa29-ba73-495a-c3d5-66a1ba1f65f5"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom PIL import Image\\nfrom torch.utils.data import Dataset, DataLoader, random_split, Subset\\nfrom torchvision import transforms, models\\nimport torchvision.transforms.functional as TF\\nfrom sklearn.model_selection import StratifiedKFold\\nimport os\\nimport time\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["'''\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed_all(42)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"wpRv5FTTDfAr","executionInfo":{"status":"ok","timestamp":1746316699386,"user_tz":240,"elapsed":8,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}},"outputId":"52777796-4599-408d-8369-15742ec5e873"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Set random seed for reproducibility\\ntorch.manual_seed(42)\\nnp.random.seed(42)\\nif torch.cuda.is_available():\\n  torch.cuda.manual_seed_all(42)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["## Configuration"],"metadata":{"id":"mP_RUlg5RBAq"}},{"cell_type":"code","source":["# Data paths\n","TRAIN_PKL_PATH = 'train.pkl'\n","TEST_PKL_PATH = 'test.pkl'\n","# MODEL_SAVE_PATH = 'best_siamese_resnet_acc.pth'\n","SUBMISSION_CSV_PATH = 'submission_siamese_resnet_acc.csv'\n","MODEL_SAVE_PATH_TEMPLATE = 'best_siamese_resnet_fold_{fold}.pth'\n","\n","# Hyperparameters\n","K_FOLDS = 5\n","BATCH_SIZE = 32 # /change 32 is better\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY = 1e-4 # can change from 1e-4 to 5e-4\n","# EPOCHS = 100\n","EPOCHS_PER_FOLD = 100 # can change to 100\n","\n","PATIENCE_LR = 3\n","PATIENCE_ES = 30\n","# VALIDATION_SPLIT = 0.2\n","USE_PRETRAINED_BASE = True # Use ImageNet weights for base ResNet\n","\n","# Inference Configuration\n","INFERENCE_BATCH_SIZE = 128\n","\n","# Device\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {DEVICE}\")\n","print(f\"K-Fold Cross-Validation with K={K_FOLDS}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"dLB8iy62_gBc","executionInfo":{"status":"ok","timestamp":1746316699391,"user_tz":240,"elapsed":4,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}},"outputId":"97922c76-d7af-48b0-c3d1-41c074c7deb1"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","K-Fold Cross-Validation with K=5\n"]}]},{"cell_type":"markdown","source":["## Data Handling"],"metadata":{"id":"BeQ-Rj20Rhr0"}},{"cell_type":"code","source":["# Data Augmentation Definition (for Training)\n","augment_transform = transforms.Compose([\n","    transforms.RandomRotation(10), #change fromn 10 to 15\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(20, padding=2), # Consider if this crop is too aggressive\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)) # Normalize single channel\n","])\n","\n","# Dataset\n","class RPSSiameseDataset(Dataset):\n","    def __init__(self, pkl_path, transform=None):\n","        self.imgs1 = None\n","        self.imgs2 = None\n","        self.labels = None\n","        self.transform = transform\n","\n","        try:\n","            with open(pkl_path, 'rb') as f:\n","                data = pickle.load(f)\n","            print(f\"Pickle file '{pkl_path}' loaded successfully.\")\n","\n","            print(\"Attempting to stack 'img1' data...\")\n","            self.imgs1 = np.stack(data['img1']).astype(np.uint8) # Stack and ensure uint8 for PIL\n","            print(f\"  'img1' stacked successfully. Shape: {self.imgs1.shape}\")\n","\n","            print(\"Attempting to stack 'img2' data...\")\n","            self.imgs2 = np.stack(data['img2']).astype(np.uint8) # Stack and ensure uint8 for PIL\n","            print(f\"  'img2' stacked successfully. Shape: {self.imgs2.shape}\")\n","\n","            labels_raw = np.array(data['label'])\n","            self.labels = torch.tensor((labels_raw == 1).astype(np.int64)) # 1 if img1 beats img2, else 0\n","\n","            assert len(self.imgs1) == len(self.labels), \"Mismatch between img1 count and labels count.\"\n","            assert len(self.imgs2) == len(self.labels), \"Mismatch between img2 count and labels count.\"\n","            assert self.imgs1.shape[1:] == (24, 24), f\"img1 shape error: {self.imgs1.shape}\"\n","            assert self.imgs2.shape[1:] == (24, 24), f\"img2 shape error: {self.imgs2.shape}\"\n","\n","            print(f\"Dataset initialized successfully from {pkl_path}: {len(self.labels)} samples.\")\n","\n","        except FileNotFoundError:\n","            print(f\"Error: File not found at {pkl_path}\")\n","        except Exception as e:\n","            print(f\"Error during dataset initialization from {pkl_path}: {e}\")\n","\n","            self.imgs1, self.imgs2, self.labels = None, None, None\n","\n","\n","    def __len__(self):\n","        return len(self.labels) if self.labels is not None else 0\n","\n","    def __getitem__(self, idx):\n","        if self.imgs1 is None or self.imgs2 is None:\n","             raise IndexError(\"Dataset not initialized correctly.\")\n","\n","        im1_pil = Image.fromarray(self.imgs1[idx]) # Already uint8 from __init__\n","        im2_pil = Image.fromarray(self.imgs2[idx]) # Already uint8 from __init__\n","        y = self.labels[idx]\n","\n","        # Apply independent transforms\n","        if self.transform:\n","            im1 = self.transform(im1_pil)\n","            im2 = self.transform(im2_pil)\n","        else:\n","            to_tensor = transforms.ToTensor()\n","            im1 = to_tensor(im1_pil)\n","            im2 = to_tensor(im2_pil)\n","\n","        return im1, im2, y\n","\n","# Dataset for Inference (Corrected Loading)\n","class RPSInferenceDataset(Dataset):\n","    def __init__(self, pkl_path, ids_key='id', img1_key='img1', img2_key='img2'):\n","        self.ids = None\n","        self.imgs1 = None\n","        self.imgs2 = None\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5,), (0.5,))\n","        ])\n","\n","        try:\n","            with open(pkl_path, 'rb') as f:\n","                 data = pickle.load(f)\n","            print(f\"Pickle file '{pkl_path}' loaded successfully for inference.\")\n","\n","            self.ids = data.get(ids_key)\n","            if self.ids is None:\n","                 raise ValueError(f\"Key '{ids_key}' not found in pickle file.\")\n","            self.ids = np.array(self.ids) # Ensure IDs are numpy array\n","\n","\n","            print(\"Attempting to stack 'img1' data for inference...\")\n","            img1_data = data.get(img1_key)\n","            if img1_data is None: raise ValueError(f\"Key '{img1_key}' not found.\")\n","            self.imgs1 = np.stack(img1_data).astype(np.uint8)\n","            print(f\"  'img1' stacked successfully. Shape: {self.imgs1.shape}\")\n","\n","\n","            print(\"Attempting to stack 'img2' data for inference...\")\n","            img2_data = data.get(img2_key)\n","            if img2_data is None: raise ValueError(f\"Key '{img2_key}' not found.\")\n","            self.imgs2 = np.stack(img2_data).astype(np.uint8)\n","            print(f\"  'img2' stacked successfully. Shape: {self.imgs2.shape}\")\n","\n","            # Validation checks\n","            assert len(self.imgs1) == len(self.ids), \"Mismatch between img1 count and ID count.\"\n","            assert len(self.imgs2) == len(self.ids), \"Mismatch between img2 count and ID count.\"\n","            assert self.imgs1.shape[1:] == (24, 24), f\"img1 shape error: {self.imgs1.shape}\"\n","            assert self.imgs2.shape[1:] == (24, 24), f\"img2 shape error: {self.imgs2.shape}\"\n","\n","            print(f\"Inference dataset initialized successfully from {pkl_path}: {len(self.ids)} samples.\")\n","\n","\n","        except FileNotFoundError:\n","            print(f\"Error: File not found at {pkl_path}\")\n","        except Exception as e:\n","            print(f\"Error during inference dataset initialization from {pkl_path}: {e}\")\n","            self.ids, self.imgs1, self.imgs2 = None, None, None\n","\n","\n","    def __len__(self):\n","        return len(self.ids) if self.ids is not None else 0\n","\n","    def __getitem__(self, idx):\n","        if self.imgs1 is None or self.imgs2 is None or self.ids is None:\n","             raise IndexError(\"Inference dataset not initialized correctly.\")\n","\n","        im1_pil = Image.fromarray(self.imgs1[idx])\n","        im2_pil = Image.fromarray(self.imgs2[idx])\n","        current_id = self.ids[idx]\n","\n","        # Apply only ToTensor and Normalize\n","        im1 = self.transform(im1_pil)\n","        im2 = self.transform(im2_pil)\n","\n","        return im1, im2, current_id\n","\n","print(\"Corrected Dataset classes defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtmjhKncRWJw","executionInfo":{"status":"ok","timestamp":1746316699429,"user_tz":240,"elapsed":28,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}},"outputId":"617fc433-8608-4f9b-aade-cde56a2e2d30"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Corrected Dataset classes defined.\n"]}]},{"cell_type":"markdown","source":["## Model Definition"],"metadata":{"id":"AMMeI6XtSBEe"}},{"cell_type":"code","source":["# Base Network (resnet18 for Feature Extractor)\n","def get_base_resnet18(pretrained=True):\n","    weights = models.ResNet18_Weights.DEFAULT if pretrained else None\n","    backbone = models.resnet18(weights=weights)\n","    original_conv1 = backbone.conv1\n","    backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","    if pretrained and original_conv1.weight.shape[1] == 3:\n","        new_weights = original_conv1.weight.data.mean(dim=1, keepdim=True)\n","        backbone.conv1.weight.data = new_weights\n","        # print(\"Adapted pretrained weights for conv1 (1 channel input).\") # Optional print\n","\n","    num_ftrs = backbone.fc.in_features\n","    backbone.fc = nn.Identity() # Remove final classification layer\n","\n","    return backbone, num_ftrs\n","\n","# Siamese Network\n","class SiameseNet(nn.Module):\n","    def __init__(self, pretrained_base=True):\n","        super().__init__()\n","        self.base_network, num_base_ftrs = get_base_resnet18(pretrained=pretrained_base)\n","        self.classifier_head = nn.Sequential(\n","            nn.Linear(num_base_ftrs * 3, 256), # /change dim\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, 2) # 2 classes for CrossEntropyLoss (0 or 1)\n","        )\n","\n","    def forward(self, input1, input2):\n","        feat1 = self.base_network(input1)\n","        feat2 = self.base_network(input2)\n","        combined_features = torch.cat([feat1, feat2, torch.abs(feat1 - feat2)], dim=1) #torch.cat((feat1, feat2), dim=1)\n","        output = self.classifier_head(combined_features)\n","        return output\n","\n","print(\"Model classes defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDm8KmfFR_Bk","executionInfo":{"status":"ok","timestamp":1746316699443,"user_tz":240,"elapsed":13,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}},"outputId":"085bcc47-7307-4a52-823b-8152b2e8c8fa"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Model classes defined.\n"]}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"75ZwDbfhSUdm"}},{"cell_type":"code","source":["def train_one_epoch(model, loader, criterion, optimizer, device):\n","    model.train() # Set model to training mode\n","    total_loss = correct = total = 0\n","    start_time = time.time()\n","    for batch_idx, (im1, im2, y) in enumerate(loader):\n","        im1, im2, y = im1.to(device), im2.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        logits = model(im1, im2)\n","        loss = criterion(logits, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * im1.size(0)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == y).sum().item()\n","        total += y.size(0)\n","\n","        # Optional: Print progress within epoch\n","        # if batch_idx % 50 == 0:\n","        #     print(f\"  Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n","\n","    epoch_time = time.time() - start_time\n","    avg_loss = total_loss / total if total > 0 else 0\n","    avg_acc = correct / total if total > 0 else 0\n","    print(f\"  Train Time: {epoch_time:.2f}s\")\n","    return avg_loss, avg_acc\n","\n","def validate(model, loader, criterion, device):\n","    model.eval() # Set model to evaluation mode\n","    total_loss = correct = total = 0\n","    with torch.no_grad(): # Disable gradient calculation\n","        for im1, im2, y in loader:\n","            im1, im2, y = im1.to(device), im2.to(device), y.to(device)\n","            logits = model(im1, im2)\n","            loss = criterion(logits, y)\n","            total_loss += loss.item() * im1.size(0)\n","            preds = logits.argmax(dim=1)\n","            correct += (preds == y).sum().item()\n","            total += y.size(0)\n","\n","    avg_loss = total_loss / total if total > 0 else 0\n","    avg_acc = correct / total if total > 0 else 0\n","    return avg_loss, avg_acc\n","\n","print(\"Training/validation functions defined.\")\n","\n","print(\"\\n Starting Training Phase\")\n","\n","# --- Cell 6: K-Fold Cross-Validation Training Phase ---\n","print(\"\\n--- Starting K-Fold Cross-Validation Training Phase ---\")\n","\n","# 6.1 Load FULL Training Data\n","full_dataset = RPSSiameseDataset(TRAIN_PKL_PATH, transform=augment_transform)\n","\n","if len(full_dataset) == 0:\n","    print(\"Training aborted: Could not load training data.\")\n","else:\n","    # Use labels from the full dataset for stratified splitting\n","    # Ensure labels are accessible, might need a small helper method in Dataset if not public\n","    # Assuming full_dataset.labels exists and is accessible:\n","    try:\n","        dataset_labels = full_dataset.labels.numpy() # Get labels as numpy array for StratifiedKFold\n","        dataset_indices = np.arange(len(full_dataset))\n","    except AttributeError:\n","         print(\"Error: Cannot access full_dataset.labels. Make sure it's accessible.\")\n","         # Handle error appropriately, maybe exit or try loading labels differently\n","         dataset_labels = None\n","\n","    if dataset_labels is not None:\n","        skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n","        fold_results = [] # Store best val_acc for each fold\n","        total_training_start_time = time.time()\n","\n","        # K-Fold Loop\n","        for fold, (train_idx, val_idx) in enumerate(skf.split(dataset_indices, dataset_labels), 1):\n","            print(f\"\\n===== Starting Fold {fold}/{K_FOLDS} =====\")\n","            fold_start_time = time.time()\n","\n","            # 6.2.1 Create Datasets and DataLoaders for the current fold\n","            train_subset = Subset(full_dataset, train_idx)\n","            val_subset = Subset(full_dataset, val_idx)\n","\n","            train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count()//2, pin_memory=True)\n","            val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count()//2, pin_memory=True)\n","            print(f\"Fold {fold}: Train batches={len(train_loader)}, Val batches={len(val_loader)}\")\n","\n","            # 6.2.2 Initialize Model, Loss, Optimizer, Scheduler FOR EACH FOLD\n","            model = SiameseNet(pretrained_base=USE_PRETRAINED_BASE).to(DEVICE)\n","            print(f\"Fold {fold}: Initialized new model instance.\")\n","            criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # Using label smoothing as suggested before\n","            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","                optimizer, mode='max', factor=0.5, patience=PATIENCE_LR, verbose=False # Set verbose=False for less output per fold\n","            )\n","\n","            # Inner Training Loop (Epoch Loop) for the current fold\n","            best_fold_val_acc = 0.0\n","            epochs_no_improve = 0\n","            fold_model_save_path = MODEL_SAVE_PATH_TEMPLATE.format(fold=fold) # Get save path for this fold\n","\n","            for epoch in range(1, EPOCHS_PER_FOLD + 1):\n","                # --- Training ---\n","                model.train() # Set train mode\n","                tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE) # Reusing train_one_epoch function\n","\n","                # --- Validation ---\n","                model.eval() # Set eval mode\n","                va_loss, va_acc = validate(model, val_loader, criterion, DEVICE) # Reusing validate function\n","\n","                print(f\"  Fold {fold} Epoch {epoch:2d} | Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.4f} | Val Loss: {va_loss:.4f}, Acc: {va_acc:.4f}\")\n","\n","                scheduler.step(va_acc)\n","\n","                # --- Early Stopping & Model Saving for the fold ---\n","                if va_acc > best_fold_val_acc:\n","                    print(f\"    ðŸš€ Fold {fold} Val Acc improved to {va_acc:.4f}. Saving model to {fold_model_save_path}\")\n","                    best_fold_val_acc = va_acc\n","                    torch.save(model.state_dict(), fold_model_save_path)\n","                    epochs_no_improve = 0\n","                else:\n","                    epochs_no_improve += 1\n","                    if epochs_no_improve >= PATIENCE_ES:\n","                        print(f\"    ðŸš¨ Fold {fold} Early stopping triggered after epoch {epoch}. Best val acc: {best_fold_val_acc:.4f}\")\n","                        break # Stop training this fold\n","\n","            # --- End of Epoch Loop for the fold ---\n","            fold_duration = time.time() - fold_start_time\n","            print(f\"===== Fold {fold} Finished in {fold_duration:.2f}s. Best Val Acc: {best_fold_val_acc:.4f} =====\")\n","            fold_results.append(best_fold_val_acc)\n","\n","        # --- End of K-Fold Loop ---\n","        total_training_duration = time.time() - total_training_start_time\n","        print(f\"\\n--- K-Fold Training Finished ---\")\n","        print(f\"Total Training Time: {total_training_duration:.2f}s\")\n","        print(f\"Validation accuracies per fold: {[f'{acc:.4f}' for acc in fold_results]}\")\n","        print(f\"Average K-Fold Validation Accuracy: {np.mean(fold_results):.4f} (+/- {np.std(fold_results):.4f})\")\n","\n","\n","print(\"\\n--- Starting Evaluation Phase (on Validation Set) ---\")\n","\n","\n","'''\n","print(\"\\n--- Starting Evaluation Phase (on Fold 1 Model's Validation Set) ---\")\n","\n","fold_to_evaluate = 1\n","eval_model_path = MODEL_SAVE_PATH_TEMPLATE.format(fold=fold_to_evaluate)\n","\n","if os.path.exists(eval_model_path):\n","    eval_model = SiameseNet(pretrained_base=USE_PRETRAINED_BASE).to(DEVICE) # Re-create model structure\n","    try:\n","        eval_model.load_state_dict(torch.load(eval_model_path, map_location=DEVICE))\n","        eval_model.eval() # Set to evaluation mode\n","        print(f\"Loaded Fold {fold_to_evaluate} model state from {eval_model_path}\")\n","\n","        # We need the validation data specific to Fold 1 again for evaluation\n","        # Re-create the split for fold 1 to get val_idx1\n","        if 'skf' in locals() and 'dataset_indices' in locals() and 'dataset_labels' in locals():\n","            # Get the indices for the fold we want to evaluate\n","            temp_train_idx, val_idx_eval = list(skf.split(dataset_indices, dataset_labels))[fold_to_evaluate-1]\n","            val_dataset_eval = Subset(full_dataset, val_idx_eval)\n","\n","            if len(val_dataset_eval) > 0:\n","                 val_eval_loader = DataLoader(val_dataset_eval, batch_size=INFERENCE_BATCH_SIZE, shuffle=False, num_workers=os.cpu_count()//2, pin_memory=True)\n","                 eval_criterion = nn.CrossEntropyLoss() # Need criterion for validate function\n","                 val_eval_loss, val_eval_acc = validate(eval_model, val_eval_loader, eval_criterion, DEVICE)\n","                 print(f\"Evaluation Accuracy on Fold {fold_to_evaluate}'s Validation Set: {val_eval_acc:.4f}\")\n","                 print(f\"Evaluation Loss on Fold {fold_to_evaluate}'s Validation Set:   {val_eval_loss:.4f}\")\n","            else:\n","                 print(f\"Could not evaluate Fold {fold_to_evaluate} - validation set empty.\")\n","        else:\n","             print(\"Cannot re-create validation split for evaluation. Required variables not found.\")\n","\n","    except Exception as e:\n","        print(f\"Error loading or evaluating Fold {fold_to_evaluate} model: {e}\")\n","else:\n","    print(f\"Skipping evaluation: Model file not found for Fold {fold_to_evaluate} at {eval_model_path}\")\n","\n","print(\"\\n--- Evaluation Phase Finished ---\")\n","'''"],"metadata":{"id":"PCJeauvYSU3u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3ae386a-6714-4c75-f692-2de76dc0e50e","executionInfo":{"status":"ok","timestamp":1746322025832,"user_tz":240,"elapsed":5326388,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Training/validation functions defined.\n","\n"," Starting Training Phase\n","\n","--- Starting K-Fold Cross-Validation Training Phase ---\n","Pickle file 'train.pkl' loaded successfully.\n","Attempting to stack 'img1' data...\n","  'img1' stacked successfully. Shape: (40000, 24, 24)\n","Attempting to stack 'img2' data...\n","  'img2' stacked successfully. Shape: (40000, 24, 24)\n","Dataset initialized successfully from train.pkl: 40000 samples.\n","\n","===== Starting Fold 1/5 =====\n","Fold 1: Train batches=500, Val batches=125\n","Fold 1: Initialized new model instance.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  Train Time: 11.97s\n","  Fold 1 Epoch  1 | Train Loss: 0.7609, Acc: 0.5307 | Val Loss: 0.7013, Acc: 0.5734\n","    ðŸš€ Fold 1 Val Acc improved to 0.5734. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.15s\n","  Fold 1 Epoch  2 | Train Loss: 0.6984, Acc: 0.5781 | Val Loss: 0.6586, Acc: 0.6176\n","    ðŸš€ Fold 1 Val Acc improved to 0.6176. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.34s\n","  Fold 1 Epoch  3 | Train Loss: 0.6655, Acc: 0.6178 | Val Loss: 0.6445, Acc: 0.6480\n","    ðŸš€ Fold 1 Val Acc improved to 0.6480. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 12.21s\n","  Fold 1 Epoch  4 | Train Loss: 0.6380, Acc: 0.6553 | Val Loss: 0.6250, Acc: 0.6709\n","    ðŸš€ Fold 1 Val Acc improved to 0.6709. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.53s\n","  Fold 1 Epoch  5 | Train Loss: 0.6188, Acc: 0.6776 | Val Loss: 0.6005, Acc: 0.6967\n","    ðŸš€ Fold 1 Val Acc improved to 0.6967. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.93s\n","  Fold 1 Epoch  6 | Train Loss: 0.6020, Acc: 0.6961 | Val Loss: 0.6010, Acc: 0.7006\n","    ðŸš€ Fold 1 Val Acc improved to 0.7006. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.47s\n","  Fold 1 Epoch  7 | Train Loss: 0.5899, Acc: 0.7089 | Val Loss: 0.5893, Acc: 0.7110\n","    ðŸš€ Fold 1 Val Acc improved to 0.7110. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.95s\n","  Fold 1 Epoch  8 | Train Loss: 0.5786, Acc: 0.7203 | Val Loss: 0.5747, Acc: 0.7196\n","    ðŸš€ Fold 1 Val Acc improved to 0.7196. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 10.95s\n","  Fold 1 Epoch  9 | Train Loss: 0.5645, Acc: 0.7337 | Val Loss: 0.5662, Acc: 0.7328\n","    ðŸš€ Fold 1 Val Acc improved to 0.7328. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.03s\n","  Fold 1 Epoch 10 | Train Loss: 0.5564, Acc: 0.7423 | Val Loss: 0.5665, Acc: 0.7258\n","  Train Time: 11.98s\n","  Fold 1 Epoch 11 | Train Loss: 0.5504, Acc: 0.7470 | Val Loss: 0.5584, Acc: 0.7370\n","    ðŸš€ Fold 1 Val Acc improved to 0.7370. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.17s\n","  Fold 1 Epoch 12 | Train Loss: 0.5429, Acc: 0.7550 | Val Loss: 0.5454, Acc: 0.7462\n","    ðŸš€ Fold 1 Val Acc improved to 0.7462. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.39s\n","  Fold 1 Epoch 13 | Train Loss: 0.5389, Acc: 0.7578 | Val Loss: 0.5458, Acc: 0.7475\n","    ðŸš€ Fold 1 Val Acc improved to 0.7475. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.48s\n","  Fold 1 Epoch 14 | Train Loss: 0.5287, Acc: 0.7678 | Val Loss: 0.5404, Acc: 0.7505\n","    ðŸš€ Fold 1 Val Acc improved to 0.7505. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.42s\n","  Fold 1 Epoch 15 | Train Loss: 0.5266, Acc: 0.7656 | Val Loss: 0.5352, Acc: 0.7585\n","    ðŸš€ Fold 1 Val Acc improved to 0.7585. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.55s\n","  Fold 1 Epoch 16 | Train Loss: 0.5232, Acc: 0.7692 | Val Loss: 0.5397, Acc: 0.7529\n","  Train Time: 11.80s\n","  Fold 1 Epoch 17 | Train Loss: 0.5156, Acc: 0.7808 | Val Loss: 0.5365, Acc: 0.7594\n","    ðŸš€ Fold 1 Val Acc improved to 0.7594. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.37s\n","  Fold 1 Epoch 18 | Train Loss: 0.5097, Acc: 0.7836 | Val Loss: 0.5291, Acc: 0.7615\n","    ðŸš€ Fold 1 Val Acc improved to 0.7615. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.73s\n","  Fold 1 Epoch 19 | Train Loss: 0.5054, Acc: 0.7860 | Val Loss: 0.5276, Acc: 0.7626\n","    ðŸš€ Fold 1 Val Acc improved to 0.7626. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.26s\n","  Fold 1 Epoch 20 | Train Loss: 0.5023, Acc: 0.7895 | Val Loss: 0.5298, Acc: 0.7629\n","    ðŸš€ Fold 1 Val Acc improved to 0.7629. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 12.13s\n","  Fold 1 Epoch 21 | Train Loss: 0.4999, Acc: 0.7923 | Val Loss: 0.5288, Acc: 0.7674\n","    ðŸš€ Fold 1 Val Acc improved to 0.7674. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 10.85s\n","  Fold 1 Epoch 22 | Train Loss: 0.4905, Acc: 0.8020 | Val Loss: 0.5188, Acc: 0.7704\n","    ðŸš€ Fold 1 Val Acc improved to 0.7704. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.68s\n","  Fold 1 Epoch 23 | Train Loss: 0.4894, Acc: 0.7989 | Val Loss: 0.5238, Acc: 0.7671\n","  Train Time: 11.49s\n","  Fold 1 Epoch 24 | Train Loss: 0.4826, Acc: 0.8048 | Val Loss: 0.5241, Acc: 0.7681\n","  Train Time: 11.26s\n","  Fold 1 Epoch 25 | Train Loss: 0.4756, Acc: 0.8108 | Val Loss: 0.5286, Acc: 0.7729\n","    ðŸš€ Fold 1 Val Acc improved to 0.7729. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 10.85s\n","  Fold 1 Epoch 26 | Train Loss: 0.4736, Acc: 0.8129 | Val Loss: 0.5184, Acc: 0.7745\n","    ðŸš€ Fold 1 Val Acc improved to 0.7745. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 12.19s\n","  Fold 1 Epoch 27 | Train Loss: 0.4683, Acc: 0.8173 | Val Loss: 0.5172, Acc: 0.7780\n","    ðŸš€ Fold 1 Val Acc improved to 0.7780. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.49s\n","  Fold 1 Epoch 28 | Train Loss: 0.4649, Acc: 0.8202 | Val Loss: 0.5292, Acc: 0.7646\n","  Train Time: 11.49s\n","  Fold 1 Epoch 29 | Train Loss: 0.4621, Acc: 0.8236 | Val Loss: 0.5208, Acc: 0.7745\n","  Train Time: 11.45s\n","  Fold 1 Epoch 30 | Train Loss: 0.4576, Acc: 0.8257 | Val Loss: 0.5226, Acc: 0.7696\n","  Train Time: 11.35s\n","  Fold 1 Epoch 31 | Train Loss: 0.4525, Acc: 0.8303 | Val Loss: 0.5212, Acc: 0.7724\n","  Train Time: 11.46s\n","  Fold 1 Epoch 32 | Train Loss: 0.4367, Acc: 0.8435 | Val Loss: 0.5148, Acc: 0.7841\n","    ðŸš€ Fold 1 Val Acc improved to 0.7841. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.04s\n","  Fold 1 Epoch 33 | Train Loss: 0.4223, Acc: 0.8533 | Val Loss: 0.5221, Acc: 0.7823\n","  Train Time: 11.86s\n","  Fold 1 Epoch 34 | Train Loss: 0.4161, Acc: 0.8587 | Val Loss: 0.5132, Acc: 0.7865\n","    ðŸš€ Fold 1 Val Acc improved to 0.7865. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.50s\n","  Fold 1 Epoch 35 | Train Loss: 0.4156, Acc: 0.8604 | Val Loss: 0.5151, Acc: 0.7916\n","    ðŸš€ Fold 1 Val Acc improved to 0.7916. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.05s\n","  Fold 1 Epoch 36 | Train Loss: 0.4104, Acc: 0.8639 | Val Loss: 0.5229, Acc: 0.7835\n","  Train Time: 11.38s\n","  Fold 1 Epoch 37 | Train Loss: 0.4084, Acc: 0.8636 | Val Loss: 0.5148, Acc: 0.7910\n","  Train Time: 11.57s\n","  Fold 1 Epoch 38 | Train Loss: 0.4042, Acc: 0.8673 | Val Loss: 0.5192, Acc: 0.7879\n","  Train Time: 11.96s\n","  Fold 1 Epoch 39 | Train Loss: 0.3985, Acc: 0.8717 | Val Loss: 0.5210, Acc: 0.7893\n","  Train Time: 11.41s\n","  Fold 1 Epoch 40 | Train Loss: 0.3890, Acc: 0.8793 | Val Loss: 0.5189, Acc: 0.7904\n","  Train Time: 11.27s\n","  Fold 1 Epoch 41 | Train Loss: 0.3841, Acc: 0.8809 | Val Loss: 0.5200, Acc: 0.7945\n","    ðŸš€ Fold 1 Val Acc improved to 0.7945. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 12.08s\n","  Fold 1 Epoch 42 | Train Loss: 0.3802, Acc: 0.8851 | Val Loss: 0.5251, Acc: 0.7879\n","  Train Time: 11.52s\n","  Fold 1 Epoch 43 | Train Loss: 0.3778, Acc: 0.8882 | Val Loss: 0.5322, Acc: 0.7871\n","  Train Time: 11.12s\n","  Fold 1 Epoch 44 | Train Loss: 0.3708, Acc: 0.8917 | Val Loss: 0.5244, Acc: 0.7866\n","  Train Time: 11.97s\n","  Fold 1 Epoch 45 | Train Loss: 0.3736, Acc: 0.8900 | Val Loss: 0.5236, Acc: 0.7926\n","  Train Time: 11.91s\n","  Fold 1 Epoch 46 | Train Loss: 0.3660, Acc: 0.8960 | Val Loss: 0.5334, Acc: 0.7920\n","  Train Time: 11.24s\n","  Fold 1 Epoch 47 | Train Loss: 0.3633, Acc: 0.8984 | Val Loss: 0.5293, Acc: 0.7899\n","  Train Time: 11.60s\n","  Fold 1 Epoch 48 | Train Loss: 0.3609, Acc: 0.8986 | Val Loss: 0.5215, Acc: 0.7969\n","    ðŸš€ Fold 1 Val Acc improved to 0.7969. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.11s\n","  Fold 1 Epoch 49 | Train Loss: 0.3601, Acc: 0.8993 | Val Loss: 0.5372, Acc: 0.7873\n","  Train Time: 11.20s\n","  Fold 1 Epoch 50 | Train Loss: 0.3573, Acc: 0.9019 | Val Loss: 0.5376, Acc: 0.7881\n","  Train Time: 11.55s\n","  Fold 1 Epoch 51 | Train Loss: 0.3579, Acc: 0.9017 | Val Loss: 0.5312, Acc: 0.7929\n","  Train Time: 11.37s\n","  Fold 1 Epoch 52 | Train Loss: 0.3596, Acc: 0.8992 | Val Loss: 0.5309, Acc: 0.7906\n","  Train Time: 11.42s\n","  Fold 1 Epoch 53 | Train Loss: 0.3520, Acc: 0.9060 | Val Loss: 0.5321, Acc: 0.7929\n","  Train Time: 11.85s\n","  Fold 1 Epoch 54 | Train Loss: 0.3495, Acc: 0.9077 | Val Loss: 0.5347, Acc: 0.7936\n","  Train Time: 11.46s\n","  Fold 1 Epoch 55 | Train Loss: 0.3505, Acc: 0.9060 | Val Loss: 0.5362, Acc: 0.7930\n","  Train Time: 11.45s\n","  Fold 1 Epoch 56 | Train Loss: 0.3494, Acc: 0.9076 | Val Loss: 0.5375, Acc: 0.7860\n","  Train Time: 11.66s\n","  Fold 1 Epoch 57 | Train Loss: 0.3487, Acc: 0.9079 | Val Loss: 0.5277, Acc: 0.7955\n","  Train Time: 11.53s\n","  Fold 1 Epoch 58 | Train Loss: 0.3458, Acc: 0.9107 | Val Loss: 0.5384, Acc: 0.7887\n","  Train Time: 11.12s\n","  Fold 1 Epoch 59 | Train Loss: 0.3444, Acc: 0.9121 | Val Loss: 0.5381, Acc: 0.7921\n","  Train Time: 11.31s\n","  Fold 1 Epoch 60 | Train Loss: 0.3468, Acc: 0.9081 | Val Loss: 0.5342, Acc: 0.7969\n","  Train Time: 11.77s\n","  Fold 1 Epoch 61 | Train Loss: 0.3421, Acc: 0.9113 | Val Loss: 0.5370, Acc: 0.7934\n","  Train Time: 11.56s\n","  Fold 1 Epoch 62 | Train Loss: 0.3431, Acc: 0.9118 | Val Loss: 0.5430, Acc: 0.7881\n","  Train Time: 11.25s\n","  Fold 1 Epoch 63 | Train Loss: 0.3431, Acc: 0.9103 | Val Loss: 0.5441, Acc: 0.7870\n","  Train Time: 10.99s\n","  Fold 1 Epoch 64 | Train Loss: 0.3469, Acc: 0.9095 | Val Loss: 0.5341, Acc: 0.7925\n","  Train Time: 11.36s\n","  Fold 1 Epoch 65 | Train Loss: 0.3435, Acc: 0.9101 | Val Loss: 0.5420, Acc: 0.7919\n","  Train Time: 11.56s\n","  Fold 1 Epoch 66 | Train Loss: 0.3429, Acc: 0.9108 | Val Loss: 0.5399, Acc: 0.7936\n","  Train Time: 11.29s\n","  Fold 1 Epoch 67 | Train Loss: 0.3414, Acc: 0.9127 | Val Loss: 0.5354, Acc: 0.7944\n","  Train Time: 11.42s\n","  Fold 1 Epoch 68 | Train Loss: 0.3426, Acc: 0.9100 | Val Loss: 0.5410, Acc: 0.7887\n","  Train Time: 11.60s\n","  Fold 1 Epoch 69 | Train Loss: 0.3412, Acc: 0.9142 | Val Loss: 0.5325, Acc: 0.7939\n","  Train Time: 11.92s\n","  Fold 1 Epoch 70 | Train Loss: 0.3453, Acc: 0.9112 | Val Loss: 0.5362, Acc: 0.7890\n","  Train Time: 11.36s\n","  Fold 1 Epoch 71 | Train Loss: 0.3387, Acc: 0.9142 | Val Loss: 0.5374, Acc: 0.7956\n","  Train Time: 11.03s\n","  Fold 1 Epoch 72 | Train Loss: 0.3408, Acc: 0.9125 | Val Loss: 0.5349, Acc: 0.7923\n","  Train Time: 11.19s\n","  Fold 1 Epoch 73 | Train Loss: 0.3417, Acc: 0.9141 | Val Loss: 0.5303, Acc: 0.7946\n","  Train Time: 11.20s\n","  Fold 1 Epoch 74 | Train Loss: 0.3419, Acc: 0.9123 | Val Loss: 0.5386, Acc: 0.7953\n","  Train Time: 11.66s\n","  Fold 1 Epoch 75 | Train Loss: 0.3429, Acc: 0.9133 | Val Loss: 0.5321, Acc: 0.7983\n","    ðŸš€ Fold 1 Val Acc improved to 0.7983. Saving model to best_siamese_resnet_fold_1.pth\n","  Train Time: 11.22s\n","  Fold 1 Epoch 76 | Train Loss: 0.3421, Acc: 0.9119 | Val Loss: 0.5367, Acc: 0.7924\n","  Train Time: 11.25s\n","  Fold 1 Epoch 77 | Train Loss: 0.3419, Acc: 0.9136 | Val Loss: 0.5367, Acc: 0.7943\n","  Train Time: 11.37s\n","  Fold 1 Epoch 78 | Train Loss: 0.3456, Acc: 0.9099 | Val Loss: 0.5346, Acc: 0.7927\n","  Train Time: 11.28s\n","  Fold 1 Epoch 79 | Train Loss: 0.3407, Acc: 0.9138 | Val Loss: 0.5364, Acc: 0.7923\n","  Train Time: 11.87s\n","  Fold 1 Epoch 80 | Train Loss: 0.3475, Acc: 0.9081 | Val Loss: 0.5421, Acc: 0.7891\n","===== Fold 1 Finished in 1058.67s. Best Val Acc: 0.7983 =====\n","\n","===== Starting Fold 2/5 =====\n","Fold 2: Train batches=500, Val batches=125\n","Fold 2: Initialized new model instance.\n","  Train Time: 11.41s\n","  Fold 2 Epoch  1 | Train Loss: 0.7481, Acc: 0.5400 | Val Loss: 0.6876, Acc: 0.5814\n","    ðŸš€ Fold 2 Val Acc improved to 0.5814. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.03s\n","  Fold 2 Epoch  2 | Train Loss: 0.6888, Acc: 0.5749 | Val Loss: 0.6589, Acc: 0.6165\n","    ðŸš€ Fold 2 Val Acc improved to 0.6165. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.47s\n","  Fold 2 Epoch  3 | Train Loss: 0.6637, Acc: 0.6129 | Val Loss: 0.6391, Acc: 0.6470\n","    ðŸš€ Fold 2 Val Acc improved to 0.6470. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.90s\n","  Fold 2 Epoch  4 | Train Loss: 0.6409, Acc: 0.6472 | Val Loss: 0.6184, Acc: 0.6726\n","    ðŸš€ Fold 2 Val Acc improved to 0.6726. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.59s\n","  Fold 2 Epoch  5 | Train Loss: 0.6227, Acc: 0.6723 | Val Loss: 0.6051, Acc: 0.6953\n","    ðŸš€ Fold 2 Val Acc improved to 0.6953. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.40s\n","  Fold 2 Epoch  6 | Train Loss: 0.6019, Acc: 0.6980 | Val Loss: 0.5905, Acc: 0.7094\n","    ðŸš€ Fold 2 Val Acc improved to 0.7094. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.76s\n","  Fold 2 Epoch  7 | Train Loss: 0.5899, Acc: 0.7063 | Val Loss: 0.5872, Acc: 0.7166\n","    ðŸš€ Fold 2 Val Acc improved to 0.7166. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.08s\n","  Fold 2 Epoch  8 | Train Loss: 0.5792, Acc: 0.7210 | Val Loss: 0.5628, Acc: 0.7352\n","    ðŸš€ Fold 2 Val Acc improved to 0.7352. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.02s\n","  Fold 2 Epoch  9 | Train Loss: 0.5670, Acc: 0.7299 | Val Loss: 0.5588, Acc: 0.7405\n","    ðŸš€ Fold 2 Val Acc improved to 0.7405. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.56s\n","  Fold 2 Epoch 10 | Train Loss: 0.5603, Acc: 0.7401 | Val Loss: 0.5555, Acc: 0.7442\n","    ðŸš€ Fold 2 Val Acc improved to 0.7442. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.29s\n","  Fold 2 Epoch 11 | Train Loss: 0.5526, Acc: 0.7445 | Val Loss: 0.5543, Acc: 0.7420\n","  Train Time: 11.68s\n","  Fold 2 Epoch 12 | Train Loss: 0.5449, Acc: 0.7524 | Val Loss: 0.5429, Acc: 0.7570\n","    ðŸš€ Fold 2 Val Acc improved to 0.7570. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.92s\n","  Fold 2 Epoch 13 | Train Loss: 0.5383, Acc: 0.7583 | Val Loss: 0.5408, Acc: 0.7545\n","  Train Time: 11.72s\n","  Fold 2 Epoch 14 | Train Loss: 0.5345, Acc: 0.7600 | Val Loss: 0.5394, Acc: 0.7562\n","  Train Time: 11.05s\n","  Fold 2 Epoch 15 | Train Loss: 0.5297, Acc: 0.7669 | Val Loss: 0.5330, Acc: 0.7632\n","    ðŸš€ Fold 2 Val Acc improved to 0.7632. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.44s\n","  Fold 2 Epoch 16 | Train Loss: 0.5202, Acc: 0.7748 | Val Loss: 0.5342, Acc: 0.7590\n","  Train Time: 11.47s\n","  Fold 2 Epoch 17 | Train Loss: 0.5176, Acc: 0.7774 | Val Loss: 0.5292, Acc: 0.7606\n","  Train Time: 11.66s\n","  Fold 2 Epoch 18 | Train Loss: 0.5118, Acc: 0.7807 | Val Loss: 0.5247, Acc: 0.7695\n","    ðŸš€ Fold 2 Val Acc improved to 0.7695. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.20s\n","  Fold 2 Epoch 19 | Train Loss: 0.5061, Acc: 0.7859 | Val Loss: 0.5260, Acc: 0.7648\n","  Train Time: 11.72s\n","  Fold 2 Epoch 20 | Train Loss: 0.4996, Acc: 0.7927 | Val Loss: 0.5268, Acc: 0.7685\n","  Train Time: 11.42s\n","  Fold 2 Epoch 21 | Train Loss: 0.4946, Acc: 0.7969 | Val Loss: 0.5204, Acc: 0.7746\n","    ðŸš€ Fold 2 Val Acc improved to 0.7746. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.26s\n","  Fold 2 Epoch 22 | Train Loss: 0.4916, Acc: 0.7998 | Val Loss: 0.5172, Acc: 0.7745\n","  Train Time: 11.43s\n","  Fold 2 Epoch 23 | Train Loss: 0.4883, Acc: 0.8035 | Val Loss: 0.5251, Acc: 0.7734\n","  Train Time: 11.19s\n","  Fold 2 Epoch 24 | Train Loss: 0.4833, Acc: 0.8039 | Val Loss: 0.5195, Acc: 0.7801\n","    ðŸš€ Fold 2 Val Acc improved to 0.7801. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.64s\n","  Fold 2 Epoch 25 | Train Loss: 0.4765, Acc: 0.8113 | Val Loss: 0.5172, Acc: 0.7761\n","  Train Time: 11.58s\n","  Fold 2 Epoch 26 | Train Loss: 0.4735, Acc: 0.8159 | Val Loss: 0.5171, Acc: 0.7785\n","  Train Time: 11.58s\n","  Fold 2 Epoch 27 | Train Loss: 0.4701, Acc: 0.8173 | Val Loss: 0.5197, Acc: 0.7745\n","  Train Time: 11.56s\n","  Fold 2 Epoch 28 | Train Loss: 0.4629, Acc: 0.8245 | Val Loss: 0.5244, Acc: 0.7719\n","  Train Time: 11.31s\n","  Fold 2 Epoch 29 | Train Loss: 0.4486, Acc: 0.8342 | Val Loss: 0.5129, Acc: 0.7834\n","    ðŸš€ Fold 2 Val Acc improved to 0.7834. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.19s\n","  Fold 2 Epoch 30 | Train Loss: 0.4367, Acc: 0.8420 | Val Loss: 0.5158, Acc: 0.7844\n","    ðŸš€ Fold 2 Val Acc improved to 0.7844. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.35s\n","  Fold 2 Epoch 31 | Train Loss: 0.4290, Acc: 0.8484 | Val Loss: 0.5100, Acc: 0.7913\n","    ðŸš€ Fold 2 Val Acc improved to 0.7913. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.50s\n","  Fold 2 Epoch 32 | Train Loss: 0.4282, Acc: 0.8497 | Val Loss: 0.5205, Acc: 0.7816\n","  Train Time: 11.30s\n","  Fold 2 Epoch 33 | Train Loss: 0.4252, Acc: 0.8537 | Val Loss: 0.5166, Acc: 0.7880\n","  Train Time: 11.29s\n","  Fold 2 Epoch 34 | Train Loss: 0.4191, Acc: 0.8557 | Val Loss: 0.5139, Acc: 0.7879\n","  Train Time: 11.33s\n","  Fold 2 Epoch 35 | Train Loss: 0.4162, Acc: 0.8584 | Val Loss: 0.5262, Acc: 0.7806\n","  Train Time: 11.16s\n","  Fold 2 Epoch 36 | Train Loss: 0.4028, Acc: 0.8688 | Val Loss: 0.5143, Acc: 0.7936\n","    ðŸš€ Fold 2 Val Acc improved to 0.7936. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.51s\n","  Fold 2 Epoch 37 | Train Loss: 0.3961, Acc: 0.8732 | Val Loss: 0.5292, Acc: 0.7864\n","  Train Time: 11.77s\n","  Fold 2 Epoch 38 | Train Loss: 0.3919, Acc: 0.8772 | Val Loss: 0.5184, Acc: 0.7907\n","  Train Time: 11.52s\n","  Fold 2 Epoch 39 | Train Loss: 0.3881, Acc: 0.8791 | Val Loss: 0.5273, Acc: 0.7880\n","  Train Time: 11.51s\n","  Fold 2 Epoch 40 | Train Loss: 0.3896, Acc: 0.8776 | Val Loss: 0.5233, Acc: 0.7867\n","  Train Time: 11.78s\n","  Fold 2 Epoch 41 | Train Loss: 0.3789, Acc: 0.8854 | Val Loss: 0.5244, Acc: 0.7873\n","  Train Time: 11.62s\n","  Fold 2 Epoch 42 | Train Loss: 0.3766, Acc: 0.8871 | Val Loss: 0.5291, Acc: 0.7851\n","  Train Time: 11.34s\n","  Fold 2 Epoch 43 | Train Loss: 0.3779, Acc: 0.8873 | Val Loss: 0.5244, Acc: 0.7896\n","  Train Time: 11.41s\n","  Fold 2 Epoch 44 | Train Loss: 0.3758, Acc: 0.8876 | Val Loss: 0.5212, Acc: 0.7909\n","  Train Time: 12.04s\n","  Fold 2 Epoch 45 | Train Loss: 0.3679, Acc: 0.8943 | Val Loss: 0.5333, Acc: 0.7889\n","  Train Time: 11.13s\n","  Fold 2 Epoch 46 | Train Loss: 0.3704, Acc: 0.8921 | Val Loss: 0.5335, Acc: 0.7943\n","    ðŸš€ Fold 2 Val Acc improved to 0.7943. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.59s\n","  Fold 2 Epoch 47 | Train Loss: 0.3711, Acc: 0.8914 | Val Loss: 0.5287, Acc: 0.7905\n","  Train Time: 11.60s\n","  Fold 2 Epoch 48 | Train Loss: 0.3679, Acc: 0.8937 | Val Loss: 0.5277, Acc: 0.7927\n","  Train Time: 11.15s\n","  Fold 2 Epoch 49 | Train Loss: 0.3654, Acc: 0.8954 | Val Loss: 0.5344, Acc: 0.7914\n","  Train Time: 11.69s\n","  Fold 2 Epoch 50 | Train Loss: 0.3666, Acc: 0.8962 | Val Loss: 0.5262, Acc: 0.7944\n","    ðŸš€ Fold 2 Val Acc improved to 0.7944. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.01s\n","  Fold 2 Epoch 51 | Train Loss: 0.3635, Acc: 0.8965 | Val Loss: 0.5343, Acc: 0.7903\n","  Train Time: 11.17s\n","  Fold 2 Epoch 52 | Train Loss: 0.3610, Acc: 0.8984 | Val Loss: 0.5322, Acc: 0.7875\n","  Train Time: 11.55s\n","  Fold 2 Epoch 53 | Train Loss: 0.3618, Acc: 0.8986 | Val Loss: 0.5292, Acc: 0.7907\n","  Train Time: 11.12s\n","  Fold 2 Epoch 54 | Train Loss: 0.3592, Acc: 0.9010 | Val Loss: 0.5336, Acc: 0.7947\n","    ðŸš€ Fold 2 Val Acc improved to 0.7947. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.33s\n","  Fold 2 Epoch 55 | Train Loss: 0.3624, Acc: 0.8990 | Val Loss: 0.5372, Acc: 0.7891\n","  Train Time: 11.92s\n","  Fold 2 Epoch 56 | Train Loss: 0.3575, Acc: 0.8987 | Val Loss: 0.5358, Acc: 0.7926\n","  Train Time: 11.18s\n","  Fold 2 Epoch 57 | Train Loss: 0.3599, Acc: 0.9001 | Val Loss: 0.5240, Acc: 0.7983\n","    ðŸš€ Fold 2 Val Acc improved to 0.7983. Saving model to best_siamese_resnet_fold_2.pth\n","  Train Time: 11.43s\n","  Fold 2 Epoch 58 | Train Loss: 0.3610, Acc: 0.8990 | Val Loss: 0.5386, Acc: 0.7900\n","  Train Time: 11.64s\n","  Fold 2 Epoch 59 | Train Loss: 0.3610, Acc: 0.8972 | Val Loss: 0.5345, Acc: 0.7893\n","  Train Time: 11.40s\n","  Fold 2 Epoch 60 | Train Loss: 0.3581, Acc: 0.9000 | Val Loss: 0.5349, Acc: 0.7864\n","  Train Time: 11.67s\n","  Fold 2 Epoch 61 | Train Loss: 0.3561, Acc: 0.9032 | Val Loss: 0.5375, Acc: 0.7853\n","  Train Time: 11.05s\n","  Fold 2 Epoch 62 | Train Loss: 0.3515, Acc: 0.9058 | Val Loss: 0.5387, Acc: 0.7844\n","  Train Time: 11.68s\n","  Fold 2 Epoch 63 | Train Loss: 0.3546, Acc: 0.9026 | Val Loss: 0.5411, Acc: 0.7834\n","  Train Time: 11.29s\n","  Fold 2 Epoch 64 | Train Loss: 0.3504, Acc: 0.9056 | Val Loss: 0.5412, Acc: 0.7865\n","  Train Time: 11.47s\n","  Fold 2 Epoch 65 | Train Loss: 0.3515, Acc: 0.9064 | Val Loss: 0.5392, Acc: 0.7876\n","  Train Time: 11.55s\n","  Fold 2 Epoch 66 | Train Loss: 0.3531, Acc: 0.9053 | Val Loss: 0.5433, Acc: 0.7893\n","  Train Time: 11.37s\n","  Fold 2 Epoch 67 | Train Loss: 0.3522, Acc: 0.9065 | Val Loss: 0.5327, Acc: 0.7919\n","  Train Time: 11.33s\n","  Fold 2 Epoch 68 | Train Loss: 0.3511, Acc: 0.9061 | Val Loss: 0.5455, Acc: 0.7884\n","  Train Time: 11.77s\n","  Fold 2 Epoch 69 | Train Loss: 0.3492, Acc: 0.9074 | Val Loss: 0.5397, Acc: 0.7907\n","  Train Time: 11.45s\n","  Fold 2 Epoch 70 | Train Loss: 0.3518, Acc: 0.9045 | Val Loss: 0.5422, Acc: 0.7867\n","  Train Time: 11.99s\n","  Fold 2 Epoch 71 | Train Loss: 0.3491, Acc: 0.9062 | Val Loss: 0.5423, Acc: 0.7886\n","  Train Time: 11.36s\n","  Fold 2 Epoch 72 | Train Loss: 0.3501, Acc: 0.9059 | Val Loss: 0.5405, Acc: 0.7917\n","  Train Time: 11.57s\n","  Fold 2 Epoch 73 | Train Loss: 0.3504, Acc: 0.9083 | Val Loss: 0.5331, Acc: 0.7950\n","  Train Time: 11.63s\n","  Fold 2 Epoch 74 | Train Loss: 0.3524, Acc: 0.9053 | Val Loss: 0.5381, Acc: 0.7924\n","  Train Time: 11.36s\n","  Fold 2 Epoch 75 | Train Loss: 0.3481, Acc: 0.9076 | Val Loss: 0.5341, Acc: 0.7927\n","  Train Time: 11.28s\n","  Fold 2 Epoch 76 | Train Loss: 0.3514, Acc: 0.9052 | Val Loss: 0.5371, Acc: 0.7940\n","  Train Time: 11.83s\n","  Fold 2 Epoch 77 | Train Loss: 0.3498, Acc: 0.9067 | Val Loss: 0.5402, Acc: 0.7881\n","  Train Time: 11.70s\n","  Fold 2 Epoch 78 | Train Loss: 0.3504, Acc: 0.9083 | Val Loss: 0.5511, Acc: 0.7841\n","  Train Time: 11.56s\n","  Fold 2 Epoch 79 | Train Loss: 0.3500, Acc: 0.9060 | Val Loss: 0.5469, Acc: 0.7819\n","  Train Time: 11.40s\n","  Fold 2 Epoch 80 | Train Loss: 0.3473, Acc: 0.9088 | Val Loss: 0.5378, Acc: 0.7904\n","===== Fold 2 Finished in 1056.23s. Best Val Acc: 0.7983 =====\n","\n","===== Starting Fold 3/5 =====\n","Fold 3: Train batches=500, Val batches=125\n","Fold 3: Initialized new model instance.\n","  Train Time: 11.05s\n","  Fold 3 Epoch  1 | Train Loss: 0.7583, Acc: 0.5359 | Val Loss: 0.7054, Acc: 0.5757\n","    ðŸš€ Fold 3 Val Acc improved to 0.5757. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.62s\n","  Fold 3 Epoch  2 | Train Loss: 0.6923, Acc: 0.5820 | Val Loss: 0.6630, Acc: 0.6222\n","    ðŸš€ Fold 3 Val Acc improved to 0.6222. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.40s\n","  Fold 3 Epoch  3 | Train Loss: 0.6664, Acc: 0.6158 | Val Loss: 0.6419, Acc: 0.6401\n","    ðŸš€ Fold 3 Val Acc improved to 0.6401. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.52s\n","  Fold 3 Epoch  4 | Train Loss: 0.6436, Acc: 0.6468 | Val Loss: 0.6220, Acc: 0.6724\n","    ðŸš€ Fold 3 Val Acc improved to 0.6724. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 12.45s\n","  Fold 3 Epoch  5 | Train Loss: 0.6204, Acc: 0.6729 | Val Loss: 0.6044, Acc: 0.6869\n","    ðŸš€ Fold 3 Val Acc improved to 0.6869. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.67s\n","  Fold 3 Epoch  6 | Train Loss: 0.6055, Acc: 0.6955 | Val Loss: 0.5970, Acc: 0.6993\n","    ðŸš€ Fold 3 Val Acc improved to 0.6993. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.87s\n","  Fold 3 Epoch  7 | Train Loss: 0.5915, Acc: 0.7078 | Val Loss: 0.5791, Acc: 0.7167\n","    ðŸš€ Fold 3 Val Acc improved to 0.7167. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.35s\n","  Fold 3 Epoch  8 | Train Loss: 0.5807, Acc: 0.7177 | Val Loss: 0.5767, Acc: 0.7244\n","    ðŸš€ Fold 3 Val Acc improved to 0.7244. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.32s\n","  Fold 3 Epoch  9 | Train Loss: 0.5708, Acc: 0.7251 | Val Loss: 0.5695, Acc: 0.7266\n","    ðŸš€ Fold 3 Val Acc improved to 0.7266. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.24s\n","  Fold 3 Epoch 10 | Train Loss: 0.5596, Acc: 0.7411 | Val Loss: 0.5625, Acc: 0.7290\n","    ðŸš€ Fold 3 Val Acc improved to 0.7290. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.82s\n","  Fold 3 Epoch 11 | Train Loss: 0.5546, Acc: 0.7449 | Val Loss: 0.5555, Acc: 0.7422\n","    ðŸš€ Fold 3 Val Acc improved to 0.7422. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.53s\n","  Fold 3 Epoch 12 | Train Loss: 0.5447, Acc: 0.7562 | Val Loss: 0.5508, Acc: 0.7475\n","    ðŸš€ Fold 3 Val Acc improved to 0.7475. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.85s\n","  Fold 3 Epoch 13 | Train Loss: 0.5398, Acc: 0.7554 | Val Loss: 0.5464, Acc: 0.7489\n","    ðŸš€ Fold 3 Val Acc improved to 0.7489. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.58s\n","  Fold 3 Epoch 14 | Train Loss: 0.5322, Acc: 0.7624 | Val Loss: 0.5422, Acc: 0.7586\n","    ðŸš€ Fold 3 Val Acc improved to 0.7586. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.50s\n","  Fold 3 Epoch 15 | Train Loss: 0.5288, Acc: 0.7669 | Val Loss: 0.5454, Acc: 0.7510\n","  Train Time: 11.70s\n","  Fold 3 Epoch 16 | Train Loss: 0.5217, Acc: 0.7738 | Val Loss: 0.5297, Acc: 0.7668\n","    ðŸš€ Fold 3 Val Acc improved to 0.7668. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 12.03s\n","  Fold 3 Epoch 17 | Train Loss: 0.5170, Acc: 0.7776 | Val Loss: 0.5402, Acc: 0.7590\n","  Train Time: 11.01s\n","  Fold 3 Epoch 18 | Train Loss: 0.5106, Acc: 0.7835 | Val Loss: 0.5267, Acc: 0.7681\n","    ðŸš€ Fold 3 Val Acc improved to 0.7681. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.69s\n","  Fold 3 Epoch 19 | Train Loss: 0.5072, Acc: 0.7855 | Val Loss: 0.5276, Acc: 0.7638\n","  Train Time: 11.80s\n","  Fold 3 Epoch 20 | Train Loss: 0.5026, Acc: 0.7897 | Val Loss: 0.5289, Acc: 0.7628\n","  Train Time: 11.17s\n","  Fold 3 Epoch 21 | Train Loss: 0.4974, Acc: 0.7966 | Val Loss: 0.5285, Acc: 0.7638\n","  Train Time: 10.96s\n","  Fold 3 Epoch 22 | Train Loss: 0.4941, Acc: 0.7963 | Val Loss: 0.5233, Acc: 0.7688\n","    ðŸš€ Fold 3 Val Acc improved to 0.7688. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.78s\n","  Fold 3 Epoch 23 | Train Loss: 0.4884, Acc: 0.8019 | Val Loss: 0.5239, Acc: 0.7691\n","    ðŸš€ Fold 3 Val Acc improved to 0.7691. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.86s\n","  Fold 3 Epoch 24 | Train Loss: 0.4833, Acc: 0.8061 | Val Loss: 0.5177, Acc: 0.7681\n","  Train Time: 11.30s\n","  Fold 3 Epoch 25 | Train Loss: 0.4766, Acc: 0.8125 | Val Loss: 0.5276, Acc: 0.7699\n","    ðŸš€ Fold 3 Val Acc improved to 0.7699. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.55s\n","  Fold 3 Epoch 26 | Train Loss: 0.4754, Acc: 0.8121 | Val Loss: 0.5172, Acc: 0.7772\n","    ðŸš€ Fold 3 Val Acc improved to 0.7772. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.34s\n","  Fold 3 Epoch 27 | Train Loss: 0.4691, Acc: 0.8182 | Val Loss: 0.5238, Acc: 0.7704\n","  Train Time: 11.82s\n","  Fold 3 Epoch 28 | Train Loss: 0.4642, Acc: 0.8226 | Val Loss: 0.5147, Acc: 0.7821\n","    ðŸš€ Fold 3 Val Acc improved to 0.7821. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.83s\n","  Fold 3 Epoch 29 | Train Loss: 0.4589, Acc: 0.8258 | Val Loss: 0.5131, Acc: 0.7782\n","  Train Time: 11.56s\n","  Fold 3 Epoch 30 | Train Loss: 0.4567, Acc: 0.8269 | Val Loss: 0.5168, Acc: 0.7794\n","  Train Time: 11.66s\n","  Fold 3 Epoch 31 | Train Loss: 0.4504, Acc: 0.8322 | Val Loss: 0.5209, Acc: 0.7798\n","  Train Time: 11.49s\n","  Fold 3 Epoch 32 | Train Loss: 0.4506, Acc: 0.8318 | Val Loss: 0.5188, Acc: 0.7766\n","  Train Time: 10.81s\n","  Fold 3 Epoch 33 | Train Loss: 0.4306, Acc: 0.8484 | Val Loss: 0.5161, Acc: 0.7863\n","    ðŸš€ Fold 3 Val Acc improved to 0.7863. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 10.97s\n","  Fold 3 Epoch 34 | Train Loss: 0.4218, Acc: 0.8547 | Val Loss: 0.5181, Acc: 0.7846\n","  Train Time: 11.70s\n","  Fold 3 Epoch 35 | Train Loss: 0.4168, Acc: 0.8585 | Val Loss: 0.5147, Acc: 0.7901\n","    ðŸš€ Fold 3 Val Acc improved to 0.7901. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 12.05s\n","  Fold 3 Epoch 36 | Train Loss: 0.4129, Acc: 0.8608 | Val Loss: 0.5216, Acc: 0.7836\n","  Train Time: 11.51s\n","  Fold 3 Epoch 37 | Train Loss: 0.4049, Acc: 0.8676 | Val Loss: 0.5311, Acc: 0.7789\n","  Train Time: 11.66s\n","  Fold 3 Epoch 38 | Train Loss: 0.4036, Acc: 0.8683 | Val Loss: 0.5184, Acc: 0.7866\n","  Train Time: 11.50s\n","  Fold 3 Epoch 39 | Train Loss: 0.4010, Acc: 0.8711 | Val Loss: 0.5260, Acc: 0.7825\n","  Train Time: 11.21s\n","  Fold 3 Epoch 40 | Train Loss: 0.3875, Acc: 0.8776 | Val Loss: 0.5294, Acc: 0.7826\n","  Train Time: 11.57s\n","  Fold 3 Epoch 41 | Train Loss: 0.3812, Acc: 0.8851 | Val Loss: 0.5299, Acc: 0.7887\n","  Train Time: 11.74s\n","  Fold 3 Epoch 42 | Train Loss: 0.3767, Acc: 0.8891 | Val Loss: 0.5247, Acc: 0.7863\n","  Train Time: 11.08s\n","  Fold 3 Epoch 43 | Train Loss: 0.3752, Acc: 0.8902 | Val Loss: 0.5356, Acc: 0.7815\n","  Train Time: 11.59s\n","  Fold 3 Epoch 44 | Train Loss: 0.3706, Acc: 0.8919 | Val Loss: 0.5273, Acc: 0.7897\n","  Train Time: 11.77s\n","  Fold 3 Epoch 45 | Train Loss: 0.3661, Acc: 0.8954 | Val Loss: 0.5259, Acc: 0.7884\n","  Train Time: 12.05s\n","  Fold 3 Epoch 46 | Train Loss: 0.3659, Acc: 0.8939 | Val Loss: 0.5231, Acc: 0.7919\n","    ðŸš€ Fold 3 Val Acc improved to 0.7919. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.35s\n","  Fold 3 Epoch 47 | Train Loss: 0.3646, Acc: 0.8962 | Val Loss: 0.5294, Acc: 0.7909\n","  Train Time: 12.26s\n","  Fold 3 Epoch 48 | Train Loss: 0.3632, Acc: 0.8974 | Val Loss: 0.5290, Acc: 0.7913\n","  Train Time: 11.42s\n","  Fold 3 Epoch 49 | Train Loss: 0.3574, Acc: 0.9022 | Val Loss: 0.5378, Acc: 0.7860\n","  Train Time: 11.74s\n","  Fold 3 Epoch 50 | Train Loss: 0.3602, Acc: 0.8976 | Val Loss: 0.5359, Acc: 0.7861\n","  Train Time: 12.12s\n","  Fold 3 Epoch 51 | Train Loss: 0.3567, Acc: 0.9006 | Val Loss: 0.5368, Acc: 0.7843\n","  Train Time: 11.67s\n","  Fold 3 Epoch 52 | Train Loss: 0.3549, Acc: 0.9035 | Val Loss: 0.5378, Acc: 0.7870\n","  Train Time: 11.58s\n","  Fold 3 Epoch 53 | Train Loss: 0.3502, Acc: 0.9060 | Val Loss: 0.5255, Acc: 0.7924\n","    ðŸš€ Fold 3 Val Acc improved to 0.7924. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.37s\n","  Fold 3 Epoch 54 | Train Loss: 0.3534, Acc: 0.9036 | Val Loss: 0.5361, Acc: 0.7904\n","  Train Time: 11.90s\n","  Fold 3 Epoch 55 | Train Loss: 0.3513, Acc: 0.9062 | Val Loss: 0.5367, Acc: 0.7887\n","  Train Time: 11.88s\n","  Fold 3 Epoch 56 | Train Loss: 0.3492, Acc: 0.9084 | Val Loss: 0.5320, Acc: 0.7919\n","  Train Time: 10.92s\n","  Fold 3 Epoch 57 | Train Loss: 0.3499, Acc: 0.9058 | Val Loss: 0.5361, Acc: 0.7931\n","    ðŸš€ Fold 3 Val Acc improved to 0.7931. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.26s\n","  Fold 3 Epoch 58 | Train Loss: 0.3457, Acc: 0.9106 | Val Loss: 0.5464, Acc: 0.7880\n","  Train Time: 11.29s\n","  Fold 3 Epoch 59 | Train Loss: 0.3491, Acc: 0.9057 | Val Loss: 0.5398, Acc: 0.7859\n","  Train Time: 11.38s\n","  Fold 3 Epoch 60 | Train Loss: 0.3454, Acc: 0.9114 | Val Loss: 0.5436, Acc: 0.7907\n","  Train Time: 11.57s\n","  Fold 3 Epoch 61 | Train Loss: 0.3457, Acc: 0.9095 | Val Loss: 0.5412, Acc: 0.7864\n","  Train Time: 11.54s\n","  Fold 3 Epoch 62 | Train Loss: 0.3477, Acc: 0.9093 | Val Loss: 0.5437, Acc: 0.7845\n","  Train Time: 11.17s\n","  Fold 3 Epoch 63 | Train Loss: 0.3453, Acc: 0.9093 | Val Loss: 0.5376, Acc: 0.7953\n","    ðŸš€ Fold 3 Val Acc improved to 0.7953. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 10.99s\n","  Fold 3 Epoch 64 | Train Loss: 0.3441, Acc: 0.9117 | Val Loss: 0.5382, Acc: 0.7875\n","  Train Time: 11.85s\n","  Fold 3 Epoch 65 | Train Loss: 0.3428, Acc: 0.9128 | Val Loss: 0.5293, Acc: 0.7943\n","  Train Time: 11.71s\n","  Fold 3 Epoch 66 | Train Loss: 0.3429, Acc: 0.9120 | Val Loss: 0.5371, Acc: 0.7933\n","  Train Time: 11.55s\n","  Fold 3 Epoch 67 | Train Loss: 0.3431, Acc: 0.9118 | Val Loss: 0.5452, Acc: 0.7890\n","  Train Time: 11.56s\n","  Fold 3 Epoch 68 | Train Loss: 0.3419, Acc: 0.9119 | Val Loss: 0.5400, Acc: 0.7876\n","  Train Time: 11.12s\n","  Fold 3 Epoch 69 | Train Loss: 0.3410, Acc: 0.9126 | Val Loss: 0.5383, Acc: 0.7899\n","  Train Time: 11.20s\n","  Fold 3 Epoch 70 | Train Loss: 0.3384, Acc: 0.9166 | Val Loss: 0.5504, Acc: 0.7855\n","  Train Time: 11.29s\n","  Fold 3 Epoch 71 | Train Loss: 0.3423, Acc: 0.9105 | Val Loss: 0.5438, Acc: 0.7964\n","    ðŸš€ Fold 3 Val Acc improved to 0.7964. Saving model to best_siamese_resnet_fold_3.pth\n","  Train Time: 11.50s\n","  Fold 3 Epoch 72 | Train Loss: 0.3418, Acc: 0.9117 | Val Loss: 0.5457, Acc: 0.7841\n","  Train Time: 12.20s\n","  Fold 3 Epoch 73 | Train Loss: 0.3431, Acc: 0.9123 | Val Loss: 0.5474, Acc: 0.7851\n","  Train Time: 11.47s\n","  Fold 3 Epoch 74 | Train Loss: 0.3384, Acc: 0.9152 | Val Loss: 0.5432, Acc: 0.7886\n","  Train Time: 11.25s\n","  Fold 3 Epoch 75 | Train Loss: 0.3442, Acc: 0.9100 | Val Loss: 0.5405, Acc: 0.7909\n","  Train Time: 11.23s\n","  Fold 3 Epoch 76 | Train Loss: 0.3375, Acc: 0.9161 | Val Loss: 0.5339, Acc: 0.7951\n","  Train Time: 11.56s\n","  Fold 3 Epoch 77 | Train Loss: 0.3412, Acc: 0.9130 | Val Loss: 0.5405, Acc: 0.7887\n","  Train Time: 11.63s\n","  Fold 3 Epoch 78 | Train Loss: 0.3400, Acc: 0.9127 | Val Loss: 0.5382, Acc: 0.7926\n","  Train Time: 11.60s\n","  Fold 3 Epoch 79 | Train Loss: 0.3393, Acc: 0.9147 | Val Loss: 0.5423, Acc: 0.7860\n","  Train Time: 11.42s\n","  Fold 3 Epoch 80 | Train Loss: 0.3374, Acc: 0.9154 | Val Loss: 0.5529, Acc: 0.7826\n","===== Fold 3 Finished in 1063.71s. Best Val Acc: 0.7964 =====\n","\n","===== Starting Fold 4/5 =====\n","Fold 4: Train batches=500, Val batches=125\n","Fold 4: Initialized new model instance.\n","  Train Time: 12.28s\n","  Fold 4 Epoch  1 | Train Loss: 0.7620, Acc: 0.5312 | Val Loss: 0.7079, Acc: 0.5766\n","    ðŸš€ Fold 4 Val Acc improved to 0.5766. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.13s\n","  Fold 4 Epoch  2 | Train Loss: 0.6990, Acc: 0.5720 | Val Loss: 0.6664, Acc: 0.6072\n","    ðŸš€ Fold 4 Val Acc improved to 0.6072. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.42s\n","  Fold 4 Epoch  3 | Train Loss: 0.6688, Acc: 0.6102 | Val Loss: 0.6439, Acc: 0.6404\n","    ðŸš€ Fold 4 Val Acc improved to 0.6404. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.15s\n","  Fold 4 Epoch  4 | Train Loss: 0.6439, Acc: 0.6422 | Val Loss: 0.6257, Acc: 0.6645\n","    ðŸš€ Fold 4 Val Acc improved to 0.6645. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.65s\n","  Fold 4 Epoch  5 | Train Loss: 0.6231, Acc: 0.6697 | Val Loss: 0.6047, Acc: 0.6911\n","    ðŸš€ Fold 4 Val Acc improved to 0.6911. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.60s\n","  Fold 4 Epoch  6 | Train Loss: 0.6082, Acc: 0.6866 | Val Loss: 0.5896, Acc: 0.7049\n","    ðŸš€ Fold 4 Val Acc improved to 0.7049. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.65s\n","  Fold 4 Epoch  7 | Train Loss: 0.5910, Acc: 0.7078 | Val Loss: 0.5807, Acc: 0.7146\n","    ðŸš€ Fold 4 Val Acc improved to 0.7146. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.65s\n","  Fold 4 Epoch  8 | Train Loss: 0.5817, Acc: 0.7134 | Val Loss: 0.5704, Acc: 0.7244\n","    ðŸš€ Fold 4 Val Acc improved to 0.7244. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.81s\n","  Fold 4 Epoch  9 | Train Loss: 0.5718, Acc: 0.7271 | Val Loss: 0.5655, Acc: 0.7298\n","    ðŸš€ Fold 4 Val Acc improved to 0.7298. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.32s\n","  Fold 4 Epoch 10 | Train Loss: 0.5627, Acc: 0.7348 | Val Loss: 0.5560, Acc: 0.7405\n","    ðŸš€ Fold 4 Val Acc improved to 0.7405. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.55s\n","  Fold 4 Epoch 11 | Train Loss: 0.5536, Acc: 0.7448 | Val Loss: 0.5481, Acc: 0.7480\n","    ðŸš€ Fold 4 Val Acc improved to 0.7480. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.39s\n","  Fold 4 Epoch 12 | Train Loss: 0.5479, Acc: 0.7526 | Val Loss: 0.5492, Acc: 0.7452\n","  Train Time: 11.18s\n","  Fold 4 Epoch 13 | Train Loss: 0.5420, Acc: 0.7538 | Val Loss: 0.5453, Acc: 0.7480\n","  Train Time: 11.90s\n","  Fold 4 Epoch 14 | Train Loss: 0.5345, Acc: 0.7605 | Val Loss: 0.5430, Acc: 0.7485\n","    ðŸš€ Fold 4 Val Acc improved to 0.7485. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.02s\n","  Fold 4 Epoch 15 | Train Loss: 0.5315, Acc: 0.7648 | Val Loss: 0.5408, Acc: 0.7511\n","    ðŸš€ Fold 4 Val Acc improved to 0.7511. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.40s\n","  Fold 4 Epoch 16 | Train Loss: 0.5237, Acc: 0.7714 | Val Loss: 0.5373, Acc: 0.7584\n","    ðŸš€ Fold 4 Val Acc improved to 0.7584. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.76s\n","  Fold 4 Epoch 17 | Train Loss: 0.5168, Acc: 0.7763 | Val Loss: 0.5343, Acc: 0.7554\n","  Train Time: 11.27s\n","  Fold 4 Epoch 18 | Train Loss: 0.5079, Acc: 0.7849 | Val Loss: 0.5307, Acc: 0.7639\n","    ðŸš€ Fold 4 Val Acc improved to 0.7639. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.70s\n","  Fold 4 Epoch 19 | Train Loss: 0.5046, Acc: 0.7883 | Val Loss: 0.5294, Acc: 0.7598\n","  Train Time: 11.41s\n","  Fold 4 Epoch 20 | Train Loss: 0.5015, Acc: 0.7914 | Val Loss: 0.5264, Acc: 0.7706\n","    ðŸš€ Fold 4 Val Acc improved to 0.7706. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.87s\n","  Fold 4 Epoch 21 | Train Loss: 0.5005, Acc: 0.7909 | Val Loss: 0.5193, Acc: 0.7779\n","    ðŸš€ Fold 4 Val Acc improved to 0.7779. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.53s\n","  Fold 4 Epoch 22 | Train Loss: 0.4933, Acc: 0.7970 | Val Loss: 0.5271, Acc: 0.7708\n","  Train Time: 11.18s\n","  Fold 4 Epoch 23 | Train Loss: 0.4881, Acc: 0.8044 | Val Loss: 0.5160, Acc: 0.7779\n","  Train Time: 11.66s\n","  Fold 4 Epoch 24 | Train Loss: 0.4851, Acc: 0.8048 | Val Loss: 0.5220, Acc: 0.7715\n","  Train Time: 11.26s\n","  Fold 4 Epoch 25 | Train Loss: 0.4801, Acc: 0.8105 | Val Loss: 0.5199, Acc: 0.7738\n","  Train Time: 11.21s\n","  Fold 4 Epoch 26 | Train Loss: 0.4625, Acc: 0.8218 | Val Loss: 0.5212, Acc: 0.7715\n","  Train Time: 11.37s\n","  Fold 4 Epoch 27 | Train Loss: 0.4546, Acc: 0.8287 | Val Loss: 0.5024, Acc: 0.7867\n","    ðŸš€ Fold 4 Val Acc improved to 0.7867. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.04s\n","  Fold 4 Epoch 28 | Train Loss: 0.4467, Acc: 0.8354 | Val Loss: 0.5125, Acc: 0.7875\n","    ðŸš€ Fold 4 Val Acc improved to 0.7875. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.73s\n","  Fold 4 Epoch 29 | Train Loss: 0.4419, Acc: 0.8383 | Val Loss: 0.5116, Acc: 0.7859\n","  Train Time: 12.19s\n","  Fold 4 Epoch 30 | Train Loss: 0.4355, Acc: 0.8430 | Val Loss: 0.5123, Acc: 0.7854\n","  Train Time: 11.54s\n","  Fold 4 Epoch 31 | Train Loss: 0.4356, Acc: 0.8423 | Val Loss: 0.5089, Acc: 0.7880\n","    ðŸš€ Fold 4 Val Acc improved to 0.7880. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.25s\n","  Fold 4 Epoch 32 | Train Loss: 0.4275, Acc: 0.8498 | Val Loss: 0.5114, Acc: 0.7901\n","    ðŸš€ Fold 4 Val Acc improved to 0.7901. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.63s\n","  Fold 4 Epoch 33 | Train Loss: 0.4271, Acc: 0.8508 | Val Loss: 0.5103, Acc: 0.7873\n","  Train Time: 11.69s\n","  Fold 4 Epoch 34 | Train Loss: 0.4197, Acc: 0.8554 | Val Loss: 0.5136, Acc: 0.7873\n","  Train Time: 11.88s\n","  Fold 4 Epoch 35 | Train Loss: 0.4168, Acc: 0.8573 | Val Loss: 0.5193, Acc: 0.7841\n","  Train Time: 11.50s\n","  Fold 4 Epoch 36 | Train Loss: 0.4120, Acc: 0.8627 | Val Loss: 0.5148, Acc: 0.7881\n","  Train Time: 10.94s\n","  Fold 4 Epoch 37 | Train Loss: 0.3999, Acc: 0.8707 | Val Loss: 0.5171, Acc: 0.7871\n","  Train Time: 11.37s\n","  Fold 4 Epoch 38 | Train Loss: 0.3944, Acc: 0.8749 | Val Loss: 0.5208, Acc: 0.7887\n","  Train Time: 12.03s\n","  Fold 4 Epoch 39 | Train Loss: 0.3905, Acc: 0.8767 | Val Loss: 0.5262, Acc: 0.7886\n","  Train Time: 11.73s\n","  Fold 4 Epoch 40 | Train Loss: 0.3882, Acc: 0.8787 | Val Loss: 0.5269, Acc: 0.7886\n","  Train Time: 11.55s\n","  Fold 4 Epoch 41 | Train Loss: 0.3839, Acc: 0.8821 | Val Loss: 0.5273, Acc: 0.7879\n","  Train Time: 11.90s\n","  Fold 4 Epoch 42 | Train Loss: 0.3775, Acc: 0.8876 | Val Loss: 0.5196, Acc: 0.7931\n","    ðŸš€ Fold 4 Val Acc improved to 0.7931. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.14s\n","  Fold 4 Epoch 43 | Train Loss: 0.3735, Acc: 0.8885 | Val Loss: 0.5312, Acc: 0.7890\n","  Train Time: 11.71s\n","  Fold 4 Epoch 44 | Train Loss: 0.3748, Acc: 0.8872 | Val Loss: 0.5279, Acc: 0.7921\n","  Train Time: 11.94s\n","  Fold 4 Epoch 45 | Train Loss: 0.3729, Acc: 0.8905 | Val Loss: 0.5224, Acc: 0.7949\n","    ðŸš€ Fold 4 Val Acc improved to 0.7949. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.71s\n","  Fold 4 Epoch 46 | Train Loss: 0.3738, Acc: 0.8901 | Val Loss: 0.5146, Acc: 0.7980\n","    ðŸš€ Fold 4 Val Acc improved to 0.7980. Saving model to best_siamese_resnet_fold_4.pth\n","  Train Time: 11.56s\n","  Fold 4 Epoch 47 | Train Loss: 0.3721, Acc: 0.8902 | Val Loss: 0.5260, Acc: 0.7941\n","  Train Time: 11.81s\n","  Fold 4 Epoch 48 | Train Loss: 0.3684, Acc: 0.8937 | Val Loss: 0.5404, Acc: 0.7860\n","  Train Time: 11.28s\n","  Fold 4 Epoch 49 | Train Loss: 0.3646, Acc: 0.8957 | Val Loss: 0.5317, Acc: 0.7900\n","  Train Time: 10.97s\n","  Fold 4 Epoch 50 | Train Loss: 0.3653, Acc: 0.8966 | Val Loss: 0.5305, Acc: 0.7911\n","  Train Time: 11.64s\n","  Fold 4 Epoch 51 | Train Loss: 0.3614, Acc: 0.8997 | Val Loss: 0.5349, Acc: 0.7911\n","  Train Time: 11.07s\n","  Fold 4 Epoch 52 | Train Loss: 0.3622, Acc: 0.8972 | Val Loss: 0.5331, Acc: 0.7897\n","  Train Time: 11.52s\n","  Fold 4 Epoch 53 | Train Loss: 0.3581, Acc: 0.9006 | Val Loss: 0.5261, Acc: 0.7933\n","  Train Time: 11.46s\n","  Fold 4 Epoch 54 | Train Loss: 0.3568, Acc: 0.9018 | Val Loss: 0.5347, Acc: 0.7887\n","  Train Time: 11.56s\n","  Fold 4 Epoch 55 | Train Loss: 0.3599, Acc: 0.9004 | Val Loss: 0.5336, Acc: 0.7864\n","  Train Time: 12.19s\n","  Fold 4 Epoch 56 | Train Loss: 0.3535, Acc: 0.9047 | Val Loss: 0.5360, Acc: 0.7924\n","  Train Time: 11.27s\n","  Fold 4 Epoch 57 | Train Loss: 0.3567, Acc: 0.9019 | Val Loss: 0.5380, Acc: 0.7919\n","  Train Time: 11.23s\n","  Fold 4 Epoch 58 | Train Loss: 0.3546, Acc: 0.9043 | Val Loss: 0.5284, Acc: 0.7974\n","  Train Time: 12.07s\n","  Fold 4 Epoch 59 | Train Loss: 0.3542, Acc: 0.9033 | Val Loss: 0.5393, Acc: 0.7887\n","  Train Time: 11.58s\n","  Fold 4 Epoch 60 | Train Loss: 0.3534, Acc: 0.9048 | Val Loss: 0.5416, Acc: 0.7875\n","  Train Time: 11.38s\n","  Fold 4 Epoch 61 | Train Loss: 0.3548, Acc: 0.9039 | Val Loss: 0.5362, Acc: 0.7900\n","  Train Time: 11.60s\n","  Fold 4 Epoch 62 | Train Loss: 0.3536, Acc: 0.9041 | Val Loss: 0.5382, Acc: 0.7907\n","  Train Time: 11.35s\n","  Fold 4 Epoch 63 | Train Loss: 0.3533, Acc: 0.9046 | Val Loss: 0.5301, Acc: 0.7939\n","  Train Time: 11.14s\n","  Fold 4 Epoch 64 | Train Loss: 0.3538, Acc: 0.9045 | Val Loss: 0.5387, Acc: 0.7876\n","  Train Time: 11.29s\n","  Fold 4 Epoch 65 | Train Loss: 0.3528, Acc: 0.9051 | Val Loss: 0.5347, Acc: 0.7956\n","  Train Time: 11.35s\n","  Fold 4 Epoch 66 | Train Loss: 0.3491, Acc: 0.9077 | Val Loss: 0.5348, Acc: 0.7924\n","  Train Time: 12.17s\n","  Fold 4 Epoch 67 | Train Loss: 0.3520, Acc: 0.9061 | Val Loss: 0.5425, Acc: 0.7863\n","  Train Time: 11.79s\n","  Fold 4 Epoch 68 | Train Loss: 0.3523, Acc: 0.9057 | Val Loss: 0.5368, Acc: 0.7955\n","  Train Time: 11.65s\n","  Fold 4 Epoch 69 | Train Loss: 0.3517, Acc: 0.9054 | Val Loss: 0.5312, Acc: 0.7883\n","  Train Time: 11.83s\n","  Fold 4 Epoch 70 | Train Loss: 0.3493, Acc: 0.9058 | Val Loss: 0.5340, Acc: 0.7901\n","  Train Time: 11.93s\n","  Fold 4 Epoch 71 | Train Loss: 0.3506, Acc: 0.9060 | Val Loss: 0.5377, Acc: 0.7906\n","  Train Time: 11.37s\n","  Fold 4 Epoch 72 | Train Loss: 0.3529, Acc: 0.9041 | Val Loss: 0.5359, Acc: 0.7941\n","  Train Time: 11.53s\n","  Fold 4 Epoch 73 | Train Loss: 0.3502, Acc: 0.9058 | Val Loss: 0.5440, Acc: 0.7910\n","  Train Time: 11.45s\n","  Fold 4 Epoch 74 | Train Loss: 0.3493, Acc: 0.9073 | Val Loss: 0.5401, Acc: 0.7897\n","  Train Time: 11.47s\n","  Fold 4 Epoch 75 | Train Loss: 0.3504, Acc: 0.9080 | Val Loss: 0.5320, Acc: 0.7914\n","  Train Time: 11.87s\n","  Fold 4 Epoch 76 | Train Loss: 0.3504, Acc: 0.9074 | Val Loss: 0.5377, Acc: 0.7935\n","  Train Time: 11.24s\n","  Fold 4 Epoch 77 | Train Loss: 0.3515, Acc: 0.9058 | Val Loss: 0.5312, Acc: 0.7957\n","  Train Time: 11.14s\n","  Fold 4 Epoch 78 | Train Loss: 0.3515, Acc: 0.9067 | Val Loss: 0.5491, Acc: 0.7847\n","  Train Time: 11.73s\n","  Fold 4 Epoch 79 | Train Loss: 0.3505, Acc: 0.9066 | Val Loss: 0.5304, Acc: 0.7933\n","  Train Time: 11.73s\n","  Fold 4 Epoch 80 | Train Loss: 0.3452, Acc: 0.9110 | Val Loss: 0.5395, Acc: 0.7884\n","===== Fold 4 Finished in 1065.09s. Best Val Acc: 0.7980 =====\n","\n","===== Starting Fold 5/5 =====\n","Fold 5: Train batches=500, Val batches=125\n","Fold 5: Initialized new model instance.\n","  Train Time: 11.69s\n","  Fold 5 Epoch  1 | Train Loss: 0.7467, Acc: 0.5457 | Val Loss: 0.7151, Acc: 0.5711\n","    ðŸš€ Fold 5 Val Acc improved to 0.5711. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.71s\n","  Fold 5 Epoch  2 | Train Loss: 0.6892, Acc: 0.5817 | Val Loss: 0.6559, Acc: 0.6144\n","    ðŸš€ Fold 5 Val Acc improved to 0.6144. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.68s\n","  Fold 5 Epoch  3 | Train Loss: 0.6565, Acc: 0.6255 | Val Loss: 0.6426, Acc: 0.6412\n","    ðŸš€ Fold 5 Val Acc improved to 0.6412. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.54s\n","  Fold 5 Epoch  4 | Train Loss: 0.6353, Acc: 0.6523 | Val Loss: 0.6286, Acc: 0.6640\n","    ðŸš€ Fold 5 Val Acc improved to 0.6640. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.63s\n","  Fold 5 Epoch  5 | Train Loss: 0.6166, Acc: 0.6768 | Val Loss: 0.6084, Acc: 0.6853\n","    ðŸš€ Fold 5 Val Acc improved to 0.6853. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.12s\n","  Fold 5 Epoch  6 | Train Loss: 0.5996, Acc: 0.6984 | Val Loss: 0.5972, Acc: 0.6980\n","    ðŸš€ Fold 5 Val Acc improved to 0.6980. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.70s\n","  Fold 5 Epoch  7 | Train Loss: 0.5877, Acc: 0.7113 | Val Loss: 0.5823, Acc: 0.7136\n","    ðŸš€ Fold 5 Val Acc improved to 0.7136. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 12.01s\n","  Fold 5 Epoch  8 | Train Loss: 0.5775, Acc: 0.7220 | Val Loss: 0.5733, Acc: 0.7275\n","    ðŸš€ Fold 5 Val Acc improved to 0.7275. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.85s\n","  Fold 5 Epoch  9 | Train Loss: 0.5683, Acc: 0.7309 | Val Loss: 0.5664, Acc: 0.7315\n","    ðŸš€ Fold 5 Val Acc improved to 0.7315. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.26s\n","  Fold 5 Epoch 10 | Train Loss: 0.5590, Acc: 0.7406 | Val Loss: 0.5601, Acc: 0.7318\n","    ðŸš€ Fold 5 Val Acc improved to 0.7318. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.43s\n","  Fold 5 Epoch 11 | Train Loss: 0.5499, Acc: 0.7489 | Val Loss: 0.5564, Acc: 0.7421\n","    ðŸš€ Fold 5 Val Acc improved to 0.7421. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.87s\n","  Fold 5 Epoch 12 | Train Loss: 0.5451, Acc: 0.7533 | Val Loss: 0.5498, Acc: 0.7470\n","    ðŸš€ Fold 5 Val Acc improved to 0.7470. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.39s\n","  Fold 5 Epoch 13 | Train Loss: 0.5357, Acc: 0.7638 | Val Loss: 0.5444, Acc: 0.7511\n","    ðŸš€ Fold 5 Val Acc improved to 0.7511. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.60s\n","  Fold 5 Epoch 14 | Train Loss: 0.5339, Acc: 0.7652 | Val Loss: 0.5424, Acc: 0.7584\n","    ðŸš€ Fold 5 Val Acc improved to 0.7584. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.28s\n","  Fold 5 Epoch 15 | Train Loss: 0.5273, Acc: 0.7680 | Val Loss: 0.5418, Acc: 0.7485\n","  Train Time: 11.20s\n","  Fold 5 Epoch 16 | Train Loss: 0.5193, Acc: 0.7729 | Val Loss: 0.5354, Acc: 0.7546\n","  Train Time: 11.41s\n","  Fold 5 Epoch 17 | Train Loss: 0.5175, Acc: 0.7789 | Val Loss: 0.5393, Acc: 0.7561\n","  Train Time: 11.44s\n","  Fold 5 Epoch 18 | Train Loss: 0.5101, Acc: 0.7825 | Val Loss: 0.5342, Acc: 0.7634\n","    ðŸš€ Fold 5 Val Acc improved to 0.7634. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.13s\n","  Fold 5 Epoch 19 | Train Loss: 0.5048, Acc: 0.7866 | Val Loss: 0.5248, Acc: 0.7692\n","    ðŸš€ Fold 5 Val Acc improved to 0.7692. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.52s\n","  Fold 5 Epoch 20 | Train Loss: 0.5009, Acc: 0.7906 | Val Loss: 0.5256, Acc: 0.7670\n","  Train Time: 11.80s\n","  Fold 5 Epoch 21 | Train Loss: 0.4975, Acc: 0.7938 | Val Loss: 0.5184, Acc: 0.7745\n","    ðŸš€ Fold 5 Val Acc improved to 0.7745. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.41s\n","  Fold 5 Epoch 22 | Train Loss: 0.4926, Acc: 0.7985 | Val Loss: 0.5182, Acc: 0.7751\n","    ðŸš€ Fold 5 Val Acc improved to 0.7751. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.57s\n","  Fold 5 Epoch 23 | Train Loss: 0.4881, Acc: 0.8033 | Val Loss: 0.5197, Acc: 0.7714\n","  Train Time: 11.35s\n","  Fold 5 Epoch 24 | Train Loss: 0.4807, Acc: 0.8083 | Val Loss: 0.5232, Acc: 0.7705\n","  Train Time: 12.08s\n","  Fold 5 Epoch 25 | Train Loss: 0.4746, Acc: 0.8117 | Val Loss: 0.5173, Acc: 0.7754\n","    ðŸš€ Fold 5 Val Acc improved to 0.7754. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 12.25s\n","  Fold 5 Epoch 26 | Train Loss: 0.4738, Acc: 0.8143 | Val Loss: 0.5239, Acc: 0.7722\n","  Train Time: 11.21s\n","  Fold 5 Epoch 27 | Train Loss: 0.4692, Acc: 0.8155 | Val Loss: 0.5164, Acc: 0.7814\n","    ðŸš€ Fold 5 Val Acc improved to 0.7814. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.66s\n","  Fold 5 Epoch 28 | Train Loss: 0.4625, Acc: 0.8233 | Val Loss: 0.5144, Acc: 0.7817\n","    ðŸš€ Fold 5 Val Acc improved to 0.7817. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.52s\n","  Fold 5 Epoch 29 | Train Loss: 0.4607, Acc: 0.8255 | Val Loss: 0.5166, Acc: 0.7786\n","  Train Time: 11.46s\n","  Fold 5 Epoch 30 | Train Loss: 0.4551, Acc: 0.8291 | Val Loss: 0.5155, Acc: 0.7776\n","  Train Time: 11.52s\n","  Fold 5 Epoch 31 | Train Loss: 0.4511, Acc: 0.8323 | Val Loss: 0.5173, Acc: 0.7742\n","  Train Time: 11.62s\n","  Fold 5 Epoch 32 | Train Loss: 0.4472, Acc: 0.8343 | Val Loss: 0.5141, Acc: 0.7873\n","    ðŸš€ Fold 5 Val Acc improved to 0.7873. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.99s\n","  Fold 5 Epoch 33 | Train Loss: 0.4411, Acc: 0.8413 | Val Loss: 0.5236, Acc: 0.7781\n","  Train Time: 11.40s\n","  Fold 5 Epoch 34 | Train Loss: 0.4357, Acc: 0.8443 | Val Loss: 0.5200, Acc: 0.7795\n","  Train Time: 11.75s\n","  Fold 5 Epoch 35 | Train Loss: 0.4343, Acc: 0.8460 | Val Loss: 0.5161, Acc: 0.7819\n","  Train Time: 11.66s\n","  Fold 5 Epoch 36 | Train Loss: 0.4312, Acc: 0.8470 | Val Loss: 0.5241, Acc: 0.7802\n","  Train Time: 10.86s\n","  Fold 5 Epoch 37 | Train Loss: 0.4099, Acc: 0.8639 | Val Loss: 0.5127, Acc: 0.7859\n","  Train Time: 11.17s\n","  Fold 5 Epoch 38 | Train Loss: 0.3986, Acc: 0.8722 | Val Loss: 0.5187, Acc: 0.7891\n","    ðŸš€ Fold 5 Val Acc improved to 0.7891. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.71s\n","  Fold 5 Epoch 39 | Train Loss: 0.3948, Acc: 0.8755 | Val Loss: 0.5250, Acc: 0.7843\n","  Train Time: 11.70s\n","  Fold 5 Epoch 40 | Train Loss: 0.3910, Acc: 0.8758 | Val Loss: 0.5263, Acc: 0.7863\n","  Train Time: 11.29s\n","  Fold 5 Epoch 41 | Train Loss: 0.3859, Acc: 0.8803 | Val Loss: 0.5355, Acc: 0.7853\n","  Train Time: 11.45s\n","  Fold 5 Epoch 42 | Train Loss: 0.3804, Acc: 0.8849 | Val Loss: 0.5236, Acc: 0.7855\n","  Train Time: 11.67s\n","  Fold 5 Epoch 43 | Train Loss: 0.3699, Acc: 0.8935 | Val Loss: 0.5272, Acc: 0.7900\n","    ðŸš€ Fold 5 Val Acc improved to 0.7900. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 12.47s\n","  Fold 5 Epoch 44 | Train Loss: 0.3656, Acc: 0.8941 | Val Loss: 0.5332, Acc: 0.7871\n","  Train Time: 11.90s\n","  Fold 5 Epoch 45 | Train Loss: 0.3598, Acc: 0.8999 | Val Loss: 0.5369, Acc: 0.7890\n","  Train Time: 11.86s\n","  Fold 5 Epoch 46 | Train Loss: 0.3568, Acc: 0.9015 | Val Loss: 0.5329, Acc: 0.7916\n","    ðŸš€ Fold 5 Val Acc improved to 0.7916. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.90s\n","  Fold 5 Epoch 47 | Train Loss: 0.3545, Acc: 0.9023 | Val Loss: 0.5298, Acc: 0.7976\n","    ðŸš€ Fold 5 Val Acc improved to 0.7976. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.44s\n","  Fold 5 Epoch 48 | Train Loss: 0.3519, Acc: 0.9061 | Val Loss: 0.5367, Acc: 0.7950\n","  Train Time: 11.63s\n","  Fold 5 Epoch 49 | Train Loss: 0.3510, Acc: 0.9067 | Val Loss: 0.5418, Acc: 0.7845\n","  Train Time: 11.51s\n","  Fold 5 Epoch 50 | Train Loss: 0.3481, Acc: 0.9096 | Val Loss: 0.5391, Acc: 0.7920\n","  Train Time: 11.62s\n","  Fold 5 Epoch 51 | Train Loss: 0.3443, Acc: 0.9101 | Val Loss: 0.5309, Acc: 0.7947\n","  Train Time: 11.79s\n","  Fold 5 Epoch 52 | Train Loss: 0.3457, Acc: 0.9097 | Val Loss: 0.5341, Acc: 0.7971\n","  Train Time: 11.51s\n","  Fold 5 Epoch 53 | Train Loss: 0.3395, Acc: 0.9150 | Val Loss: 0.5322, Acc: 0.7990\n","    ðŸš€ Fold 5 Val Acc improved to 0.7990. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.97s\n","  Fold 5 Epoch 54 | Train Loss: 0.3344, Acc: 0.9178 | Val Loss: 0.5376, Acc: 0.7895\n","  Train Time: 11.67s\n","  Fold 5 Epoch 55 | Train Loss: 0.3347, Acc: 0.9184 | Val Loss: 0.5475, Acc: 0.7900\n","  Train Time: 12.08s\n","  Fold 5 Epoch 56 | Train Loss: 0.3323, Acc: 0.9192 | Val Loss: 0.5309, Acc: 0.7973\n","  Train Time: 11.68s\n","  Fold 5 Epoch 57 | Train Loss: 0.3306, Acc: 0.9210 | Val Loss: 0.5509, Acc: 0.7904\n","  Train Time: 11.63s\n","  Fold 5 Epoch 58 | Train Loss: 0.3320, Acc: 0.9196 | Val Loss: 0.5333, Acc: 0.7959\n","  Train Time: 11.61s\n","  Fold 5 Epoch 59 | Train Loss: 0.3294, Acc: 0.9224 | Val Loss: 0.5402, Acc: 0.8003\n","    ðŸš€ Fold 5 Val Acc improved to 0.8003. Saving model to best_siamese_resnet_fold_5.pth\n","  Train Time: 11.62s\n","  Fold 5 Epoch 60 | Train Loss: 0.3302, Acc: 0.9201 | Val Loss: 0.5488, Acc: 0.7913\n","  Train Time: 11.51s\n","  Fold 5 Epoch 61 | Train Loss: 0.3241, Acc: 0.9232 | Val Loss: 0.5433, Acc: 0.7947\n","  Train Time: 12.19s\n","  Fold 5 Epoch 62 | Train Loss: 0.3254, Acc: 0.9237 | Val Loss: 0.5404, Acc: 0.7975\n","  Train Time: 11.72s\n","  Fold 5 Epoch 63 | Train Loss: 0.3273, Acc: 0.9232 | Val Loss: 0.5466, Acc: 0.7955\n","  Train Time: 11.68s\n","  Fold 5 Epoch 64 | Train Loss: 0.3223, Acc: 0.9254 | Val Loss: 0.5447, Acc: 0.7899\n","  Train Time: 11.48s\n","  Fold 5 Epoch 65 | Train Loss: 0.3249, Acc: 0.9236 | Val Loss: 0.5480, Acc: 0.7891\n","  Train Time: 11.54s\n","  Fold 5 Epoch 66 | Train Loss: 0.3230, Acc: 0.9245 | Val Loss: 0.5410, Acc: 0.7913\n","  Train Time: 11.77s\n","  Fold 5 Epoch 67 | Train Loss: 0.3217, Acc: 0.9274 | Val Loss: 0.5430, Acc: 0.7891\n","  Train Time: 11.28s\n","  Fold 5 Epoch 68 | Train Loss: 0.3198, Acc: 0.9291 | Val Loss: 0.5507, Acc: 0.7903\n","  Train Time: 11.39s\n","  Fold 5 Epoch 69 | Train Loss: 0.3225, Acc: 0.9260 | Val Loss: 0.5449, Acc: 0.7957\n","  Train Time: 11.90s\n","  Fold 5 Epoch 70 | Train Loss: 0.3176, Acc: 0.9298 | Val Loss: 0.5435, Acc: 0.7945\n","  Train Time: 11.71s\n","  Fold 5 Epoch 71 | Train Loss: 0.3229, Acc: 0.9274 | Val Loss: 0.5440, Acc: 0.7931\n","  Train Time: 12.41s\n","  Fold 5 Epoch 72 | Train Loss: 0.3202, Acc: 0.9267 | Val Loss: 0.5460, Acc: 0.7929\n","  Train Time: 11.99s\n","  Fold 5 Epoch 73 | Train Loss: 0.3191, Acc: 0.9309 | Val Loss: 0.5361, Acc: 0.7990\n","  Train Time: 11.79s\n","  Fold 5 Epoch 74 | Train Loss: 0.3217, Acc: 0.9264 | Val Loss: 0.5436, Acc: 0.7947\n","  Train Time: 11.53s\n","  Fold 5 Epoch 75 | Train Loss: 0.3203, Acc: 0.9278 | Val Loss: 0.5460, Acc: 0.7906\n","  Train Time: 11.72s\n","  Fold 5 Epoch 76 | Train Loss: 0.3180, Acc: 0.9305 | Val Loss: 0.5455, Acc: 0.7901\n","  Train Time: 11.53s\n","  Fold 5 Epoch 77 | Train Loss: 0.3214, Acc: 0.9266 | Val Loss: 0.5453, Acc: 0.7923\n","  Train Time: 11.64s\n","  Fold 5 Epoch 78 | Train Loss: 0.3210, Acc: 0.9280 | Val Loss: 0.5389, Acc: 0.7985\n","  Train Time: 11.60s\n","  Fold 5 Epoch 79 | Train Loss: 0.3196, Acc: 0.9272 | Val Loss: 0.5446, Acc: 0.7936\n","  Train Time: 11.47s\n","  Fold 5 Epoch 80 | Train Loss: 0.3196, Acc: 0.9287 | Val Loss: 0.5481, Acc: 0.7904\n","===== Fold 5 Finished in 1070.86s. Best Val Acc: 0.8003 =====\n","\n","--- K-Fold Training Finished ---\n","Total Training Time: 5314.61s\n","Validation accuracies per fold: ['0.7983', '0.7983', '0.7964', '0.7980', '0.8003']\n","Average K-Fold Validation Accuracy: 0.7982 (+/- 0.0012)\n","\n","--- Starting Evaluation Phase (on Validation Set) ---\n","\n","--- Starting Evaluation Phase (on Fold 1 Model's Validation Set) ---\n","Loaded Fold 1 model state from best_siamese_resnet_fold_1.pth\n","Evaluation Accuracy on Fold 1's Validation Set: 0.7944\n","Evaluation Loss on Fold 1's Validation Set:   0.4593\n","\n","--- Evaluation Phase Finished ---\n"]}]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"IeOPzHrDS-og"}},{"cell_type":"code","source":["# --- Cell 8: Inference Phase (Ensemble Prediction) ---\n","print(\"\\n--- Starting Inference Phase (Ensemble Prediction) ---\")\n","\n","# --- MODIFICATION START ---\n","# 8.1 Load ALL K Models\n","models_for_inference = []\n","for fold in range(1, K_FOLDS + 1):\n","    model_path = MODEL_SAVE_PATH_TEMPLATE.format(fold=fold)\n","    if os.path.exists(model_path):\n","        model = SiameseNet(pretrained_base=USE_PRETRAINED_BASE).to(DEVICE)\n","        try:\n","            model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n","            model.eval() # Set to evaluation mode\n","            models_for_inference.append(model)\n","            print(f\"Loaded model from {model_path} for inference.\")\n","        except Exception as e:\n","            print(f\"Warning: Error loading model {model_path}: {e}. Skipping this model.\")\n","    else:\n","        print(f\"Warning: Model file not found at {model_path}. Skipping this model for ensemble.\")\n","\n","if not models_for_inference:\n","    print(\"Cannot perform inference: No trained models loaded.\")\n","else:\n","    print(f\"Loaded {len(models_for_inference)} models for ensemble inference.\")\n","    # --- MODIFICATION END ---\n","\n","    # 8.2 Load Test Data\n","    test_dataset = RPSInferenceDataset(TEST_PKL_PATH)\n","    if len(test_dataset) > 0:\n","        test_loader = DataLoader(test_dataset, batch_size=INFERENCE_BATCH_SIZE, shuffle=False, num_workers=os.cpu_count()//2)\n","        print(f\"Test DataLoader created: {len(test_loader)} batches.\")\n","\n","        # 8.3 Perform Inference with Ensemble\n","        all_preds_ensemble = []\n","        all_ids = []\n","        inference_start_time = time.time()\n","        with torch.no_grad():\n","            for im1, im2, ids_batch in test_loader:\n","                im1, im2 = im1.to(DEVICE), im2.to(DEVICE)\n","\n","                # --- MODIFICATION START ---\n","                batch_logits_list = []\n","                # Get predictions from each model in the ensemble\n","                for model in models_for_inference:\n","                    logits = model(im1, im2)\n","                    batch_logits_list.append(logits)\n","\n","                # Average the logits (or probabilities if you prefer) across models\n","                # Stacking logits: (K, B, Num_Classes) -> Averaging over K: (B, Num_Classes)\n","                avg_logits = torch.stack(batch_logits_list).mean(dim=0)\n","\n","                # Get final prediction from averaged logits\n","                preds = avg_logits.argmax(dim=1).cpu().numpy()\n","                # --- MODIFICATION END ---\n","\n","                all_preds_ensemble.append(preds)\n","                all_ids.extend(ids_batch.numpy() if isinstance(ids_batch, torch.Tensor) else ids_batch)\n","\n","        inference_duration = time.time() - inference_start_time\n","        print(f\"Ensemble inference completed in {inference_duration:.2f}s\")\n","\n","        if all_preds_ensemble:\n","             final_preds = np.concatenate(all_preds_ensemble) # Rename variable for clarity\n","        else:\n","             final_preds = np.array([])\n","             print(\"Warning: No predictions were generated.\")\n","\n","        # --- Cell 9: Post-processing & Submission (No changes needed, uses final_preds) ---\n","        print(\"\\n--- Starting Post-processing & Submission ---\")\n","        # (Cell 9 code remains the same, it will use the 'final_preds' calculated above)\n","        # ... ensure it uses SUBMISSION_CSV_PATH = 'submission_siamese_resnet_cv_ensemble.csv' ...\n","        if 'final_preds' in locals() and len(final_preds) == len(all_ids) and len(final_preds) > 0 :\n","            # Map predictions (0/1) back to labels (-1/+1)\n","            final_labels = np.where(final_preds == 1, 1, -1)\n","            # Create submission DataFrame\n","            submission_df = pd.DataFrame({'id': all_ids, 'label': final_labels})\n","            # Save to CSV\n","            try:\n","                submission_df.to_csv(SUBMISSION_CSV_PATH, index=False)\n","                print(f\"âœ… Submission file saved successfully to: {SUBMISSION_CSV_PATH}\")\n","                print(\"\\nSubmission file preview:\")\n","                print(submission_df.head())\n","            except Exception as e:\n","                print(f\"Error saving submission file: {e}\")\n","        elif 'final_preds' in locals():\n","            print(f\"Error: Number of predictions ({len(final_preds)}) does not match number of IDs ({len(all_ids)}). Cannot create submission file.\")\n","        else:\n","            print(\"Error: No final predictions available to save.\")\n","\n","    else:\n","        print(\"Skipping inference: Test dataset could not be loaded or is empty.\")\n","\n","print(\"\\n--- Notebook Execution Finished ---\")"],"metadata":{"id":"jKssoLbKS_6z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746322036809,"user_tz":240,"elapsed":10940,"user":{"displayName":"Haning Song","userId":"09165307899440456843"}},"outputId":"be9f1e0f-ac50-43d2-8dea-3cb6c0bd8a4b"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Starting Inference Phase (Ensemble Prediction) ---\n","Loaded model from best_siamese_resnet_fold_1.pth for inference.\n","Loaded model from best_siamese_resnet_fold_2.pth for inference.\n","Loaded model from best_siamese_resnet_fold_3.pth for inference.\n","Loaded model from best_siamese_resnet_fold_4.pth for inference.\n","Loaded model from best_siamese_resnet_fold_5.pth for inference.\n","Loaded 5 models for ensemble inference.\n","Pickle file 'test.pkl' loaded successfully for inference.\n","Attempting to stack 'img1' data for inference...\n","  'img1' stacked successfully. Shape: (20000, 24, 24)\n","Attempting to stack 'img2' data for inference...\n","  'img2' stacked successfully. Shape: (20000, 24, 24)\n","Inference dataset initialized successfully from test.pkl: 20000 samples.\n","Test DataLoader created: 157 batches.\n","Ensemble inference completed in 5.34s\n","\n","--- Starting Post-processing & Submission ---\n","âœ… Submission file saved successfully to: submission_siamese_resnet_acc.csv\n","\n","Submission file preview:\n","     id  label\n","0  3386     -1\n","1   962      1\n","2  6965      1\n","3  3746     -1\n","4  7308     -1\n","\n","--- Notebook Execution Finished ---\n"]}]}]}